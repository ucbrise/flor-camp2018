{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Flor Tutorial!\n",
    "\n",
    "\n",
    "* Using Flor for lightweight versioning and tracking of ML/AI experiments.\n",
    "\n",
    "* How to collaborate using Flor \n",
    "\n",
    "* What happens with more detailed Flor pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Annotating and Versioning\n",
    "\n",
    "## Prepare your environment before starting the activities.\n",
    "\n",
    "We're going to start by importing Flor and letting it know the name of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flor\n",
    "import flor\n",
    "\n",
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('pinterest_demo_nc.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "* We're going to train a model to predict the sentiment -- positive, or negative -- of English phrases.\n",
    "\n",
    "The pipeline is pretty standard: \n",
    "\n",
    "* load the dataset\n",
    "* do some light preprocessing \n",
    "* do a train/test split\n",
    "* train the model\n",
    "* validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Random Forest Classifier with 5 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Data\n",
    "movie_reviews = pd.read_json('data.json')\n",
    "\n",
    "# Do light preprocessing\n",
    "movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "# Do train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                          test_size=0.20, random_state=92)\n",
    "\n",
    "# Vectorize the English sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_tr)\n",
    "X_tr = vectorizer.transform(X_tr)\n",
    "X_te = vectorizer.transform(X_te)\n",
    "\n",
    "# Fit the model              ##############\n",
    "clf = RandomForestClassifier(n_estimators=5).fit(X_tr, y_tr)\n",
    "                             ##############\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "# Validate the predictions\n",
    "c = classification_report(y_te, y_pred)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we learn? \n",
    "\n",
    "### Versioning \n",
    "\n",
    "* We did not overwrite the previous Jupyter cell to preserve the history of our experimentation \n",
    "\n",
    "### Experiment Annotation\n",
    "\n",
    "* We used inline documentation and code comments to highlight key features \n",
    "\n",
    "### but... this relied on the user adhering to \"best practices\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Flor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "@flor.track_action('demo')\n",
    "def split_train_and_eval(**kwargs):\n",
    "###################################\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    movie_reviews = pd.read_json('data.json')\n",
    "    movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                              test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    clf = RandomForestClassifier(n_estimators=5).fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    c = classification_report(y_te, y_pred)\n",
    "\n",
    "    print(c)\n",
    "    \n",
    "    #######################\n",
    "    return {'score': score}\n",
    "    #######################\n",
    "\n",
    "# This is how we run the pipeline\n",
    "split_train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try evaluating n_estimators at 7 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter sweeps in Flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.track_action('demo')\n",
    "                         ############  #########\n",
    "def split_train_and_eval(n_estimators, max_depth, **kwargs):\n",
    "                         ############  #########\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    movie_reviews = pd.read_json('data.json')\n",
    "    movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                              test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)         ############            #########\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth).fit(X_tr, y_tr)\n",
    "                                              ############            #########\n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    c = classification_report(y_te, y_pred)\n",
    "\n",
    "    print(c)\n",
    "    \n",
    "    return {'score': score}\n",
    "    \n",
    "# This is how we run the pipeline\n",
    "split_train_and_eval(n_estimators=7, max_depth=[10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sharing and Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting someone else's work in Flor\n",
    "\n",
    "* Our friend Bob is a Flor user and already did some preprocessing work in his experiment 'bob_preproc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are two past versions of the experiment bob_preproc: first_preproc, and second_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').plot('first_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the structure of the Flor Plan for Bob's preprocessing expriment. We see that there are four (4) rectangles: \n",
    "* Inputs: `preprocess`, `data_loc`\n",
    "* Outputs: `intermediate_X`, and `intermediate_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').plot('second_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both node-link diagrams look the same. This means that the structure of the different experiment versions is the same; however, it is very likely that the contents of the computation graph differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').diff('first_preproc', 'second_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to inspect the differences in more detail, we can do so by controlling the flags to `diff`. Below we add the flag `--minimal`, but you're welcome to try [other flags](https://git-scm.com/docs/git-diff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').diff('first_preproc', 'second_preproc', flags=['--minimal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond the Basic Flor API: Flor's Extended API\n",
    "We previously used the `flor.track_action` decorator to wrap a pipeline and execute it with a minimum of extra Flor wrapper code. `track_action` infers a simple underlying Flor Plan for our code. We refer to functions wrapped with `track_action` as Flor's *basic* API. \n",
    "\n",
    "We wrap our functions with the `flor.func` decorator. This requires the developer to explicitly declare a Flor Plan. This allows us to represent pipelines with __many Actions__, and enables us to __re-use the Artifacts of someone else's experiment__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bob's work in Flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we define the split_train_and_eval function. Running this cell will not execute the experiment because we will need to specify a Flor plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "@flor.func\n",
    "##########               ##############  ##############\n",
    "def split_train_and_eval(intermediate_X, intermediate_y, n_estimators, max_depth, **kwargs):\n",
    "                         ##############  ##############\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "            \n",
    "              ##############\n",
    "    with open(intermediate_X) as json_data:\n",
    "              ##############\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "        \n",
    "              ##############\n",
    "    with open(intermediate_y) as json_data:\n",
    "              ##############\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth).fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    print(score)\n",
    "    \n",
    "    return {'score': score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define the Flor plan \n",
    "\n",
    "* Notice that we are able to maintain context of multiple experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('bob_preproc') as bob, flor.Experiment('demo') as ex:\n",
    "    # This is how we tell Flor we will be using Bob's derived artifacts\n",
    "    data_x = bob.artifact('data_clean_X.json', 'intermediate_X', label=\"first_preproc\")\n",
    "    data_y = bob.artifact('data_clean_y.json', 'intermediate_y', label=\"first_preproc\")\n",
    "    \n",
    "    # This is how we specify the \"literals\" (parameters) of the Flor Plan for our experiments     \n",
    "    n_estimators = ex.literal(7, 'n_estimators') # The best parameter from the previous experiment\n",
    "    max_depth = ex.literal(100, 'max_depth')\n",
    "                                                                \n",
    "    # This is where we put the code we declared in the cell above\n",
    "    do_split_train_and_eval = ex.action(split_train_and_eval, [data_x, data_y, n_estimators, max_depth])\n",
    "    score = ex.literal(name='score', parent=do_split_train_and_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's verify that everything looks as it should. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.pull('fifth_pull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despite using the best configuration from the previous experiment, our score dropped to 65% :( \n",
    "\n",
    "* The only change we made was using Bob's preprocessed data and we tried out the first version already. Let's try using the second version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('bob_preproc').summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.version = \"second_preproc\"\n",
    "data_y.version = \"second_preproc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.pull('sixth_pull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Flor Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def split(intermediate_X, intermediate_y, test_size, random_state, \n",
    "          X_train, X_test, y_train, y_test, **kwargs):\n",
    "    import json\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "        \n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "        \n",
    "    # Split the data\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Write the outputs\n",
    "    with open(X_train, 'w') as f:\n",
    "        json.dump(X_tr, f)\n",
    "    with open(X_test, 'w') as f:\n",
    "        json.dump(X_te, f)\n",
    "    with open(y_train, 'w') as f:\n",
    "        json.dump(y_tr, f)\n",
    "    with open(y_test, 'w') as f:\n",
    "        json.dump(y_te, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train(X_train, y_train, n_estimators, max_depth, model, vectorizer, **kwargs):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import cloudpickle\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(X_train, 'r') as f:\n",
    "        X_tr = json.load(f)\n",
    "    with open(y_train, 'r') as f:\n",
    "        y_tr = json.load(f)\n",
    "    \n",
    "    # Fit the vectorizer\n",
    "    vec = TfidfVectorizer()\n",
    "    vec.fit(X_tr)\n",
    "    \n",
    "    # Transform the training data\n",
    "    X_tr = vec.transform(X_tr)\n",
    "    \n",
    "    # Train the model\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth).fit(X_tr, y_tr)\n",
    "    \n",
    "    # Write the output\n",
    "    with open(model, 'wb') as f:\n",
    "        cloudpickle.dump(clf, f)\n",
    "    with open(vectorizer, 'wb') as f:\n",
    "        cloudpickle.dump(vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def eval(X_test, y_test, model, vectorizer, **kwargs):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import cloudpickle\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(X_test, 'r') as f:\n",
    "        X_te = json.load(f)\n",
    "    with open(y_test, 'r') as f:\n",
    "        y_te = json.load(f)\n",
    "    with open(model, 'rb') as f:\n",
    "        clf = cloudpickle.load(f)\n",
    "    with open(vectorizer, 'rb') as f:\n",
    "        vec = cloudpickle.load(f)\n",
    "    \n",
    "    # Test the model\n",
    "    X_te = vec.transform(X_te)\n",
    "    score = clf.score(X_te, y_te)\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    # Return the score\n",
    "    return {'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('bob_preproc') as bob, flor.Experiment('demo') as ex:\n",
    "    # This is how we tell Flor we will be using Bob's derived artifacts\n",
    "    data_x = bob.artifact('data_clean_X.json', 'intermediate_X', label=\"second_preproc\")\n",
    "    data_y = bob.artifact('data_clean_y.json', 'intermediate_y', label=\"second_preproc\")\n",
    "    \n",
    "    # Here we declare all the static literals\n",
    "    random_state = ex.literal(92, 'random_state')\n",
    "    test_size = ex.literal(0.20, 'test_size')\n",
    "    n_estimators = ex.literal(7, 'n_estimators') \n",
    "    max_depth = ex.literal(100, 'max_depth')\n",
    "    \n",
    "    # Now we connect the Flor Plan\n",
    "    do_split = ex.action(func=split, in_artifacts=[data_x, data_y, test_size, random_state])\n",
    "    X_train = ex.artifact(loc='X_train.json', name='X_train', parent=do_split)\n",
    "    X_test = ex.artifact(loc='X_test.json', name='X_test', parent=do_split)\n",
    "    y_train = ex.artifact(loc='y_train.json', name='y_train', parent=do_split)\n",
    "    y_test = ex.artifact(loc='y_test.json', name='y_test', parent=do_split)\n",
    "    \n",
    "    do_train = ex.action(func=train, in_artifacts=[X_train, y_train, n_estimators, max_depth])\n",
    "    model = ex.artifact(loc='model.pkl', name='model', parent=do_train)\n",
    "    vectorizer = ex.artifact(loc='vectorizer.pkl', name='vectorizer', parent=do_train)\n",
    "    \n",
    "    do_eval = ex.action(func=eval, in_artifacts=[X_test, y_test, model, vectorizer])\n",
    "    score = ex.literal(name='score', parent=do_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.pull('seventh_pull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing models from previous Flor experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('demo') as ex:\n",
    "    model = ex.artifact('model.pkl', 'model', label='seventh_pull')\n",
    "    vectorizer = ex.artifact('vectorizer.pkl', 'vectorizer', label='seventh_pull')\n",
    "model = model.peek()\n",
    "vec = vectorizer.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"What's on your mind? \"\n",
    "\n",
    "phrase = input(PROMPT)\n",
    "while phrase[0:len('nothing')].lower() != 'nothing':\n",
    "    phrase = vec.transform([phrase,])\n",
    "    positive = model.predict(phrase)\n",
    "    if positive:\n",
    "        print('Happy to hear that!\\n'.format(phrase))\n",
    "    else:\n",
    "        print(\"Sorry about that...\\n\")\n",
    "    phrase = input(PROMPT)\n",
    "print('you said nothing.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
