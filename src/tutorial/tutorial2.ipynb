{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp 2018: Introduction to Flor!\n",
    "\n",
    "\n",
    "Welcome to RISE Camp 2018! Flor is a system for managing workflow development within the machine learning lifecycle. This tool enables data scientists to describe ML workflows as directed acyclic graphs (DAGs) of Actions, Artifacts, or Literals and to experiment with different configurations quickly by running multi-trial experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![object model](./object_model.png)\n",
    "<center><i>Artifacts in rectangles; Literals in underline; Actions in circles</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to help you use Flor in order to navigate through different parts of the data science lifecycle.\n",
    "\n",
    "As you work through this notebook, you will learn:\n",
    "\n",
    "* How to define/use experiments, literals, artifacts and actions.\n",
    "* How to run experiments with different configurations.\n",
    "* Compare models with other past versions in order to select the best model.\n",
    "\n",
    "We will be working with a ratings dataset. Our goal is to predict whether a movie review is positive or negative based on its text.\n",
    "\n",
    "**Data science is a collaborative activity - we encourage you to work with those around you and ask questions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your environment before starting the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flor\n",
    "import flor\n",
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import florfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please make sure that `tutorial/florfunctions.py` is open alongside this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_json('data.json')\n",
    "movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], test_size=0.20, random_state=92)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_tr)\n",
    "X_tr = vectorizer.transform(X_tr)\n",
    "X_te = vectorizer.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=5).fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "score = clf.score(X_te, y_te)\n",
    "c = classification_report(y_te, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=7).fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "score = clf.score(X_te, y_te)\n",
    "c = classification_report(y_te, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def split_train_and_eval(intermediate_X, intermediate_y, **kwargs):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "#     movie_reviews = pd.read_json('data.json')\n",
    "#     movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    clf = RandomForestClassifier(n_estimators=5).fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    c = classification_report(y_te, y_pred)\n",
    "\n",
    "    print(score)\n",
    "    print(c)\n",
    "    \n",
    "    return {'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag=\"second_preproc\")\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag=\"second_preproc\")\n",
    "    \n",
    "with flor.Experiment('flor_intro') as ex:\n",
    "    do_split_train_and_eval = ex.action(split_train_and_eval, [data_x, data_y])\n",
    "    score = ex.literal(name='score', parent=do_split_train_and_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: source Pages: 1 -->\n",
       "<svg width=\"307pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 307.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>source</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 303,-184 303,4 -4,4\"/>\n",
       "<!-- 2 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"57,-180 0,-180 0,-144 57,-144 57,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"28.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tutorial</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127.5\" cy=\"-90\" rx=\"86.3847\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">split_train_and_eval</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M53.4819,-143.8314C66.1947,-134.5857 81.7941,-123.2407 95.3888,-113.3536\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.5219,-116.1301 103.5506,-107.4177 93.4047,-110.4689 97.5219,-116.1301\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"179.5,-180 75.5,-180 75.5,-144 179.5,-144 179.5,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">intermediate_X</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M127.5,-143.8314C127.5,-136.131 127.5,-126.9743 127.5,-118.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.0001,-118.4132 127.5,-108.4133 124.0001,-118.4133 131.0001,-118.4132\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"299,-180 198,-180 198,-144 299,-144 299,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">intermediate_y</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M217.9666,-143.8314C201.8697,-134.253 181.9864,-122.4217 164.9635,-112.2923\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.7411,-109.2773 156.3576,-107.1715 163.1615,-115.2929 166.7411,-109.2773\"/>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"154.5,-36 100.5,-36 100.5,0 154.5,0 154.5,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"100.5,0 154.5,0 \"/>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">score</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M127.5,-71.8314C127.5,-64.131 127.5,-54.9743 127.5,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.0001,-46.4132 127.5,-36.4133 124.0001,-46.4133 131.0001,-46.4132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f3bd0bb2e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.71      0.72      5024\n",
      "          1       0.72      0.73      0.72      4976\n",
      "\n",
      "avg / total       0.72      0.72      0.72     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score.pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utag</th>\n",
       "      <th>score</th>\n",
       "      <th>intermediate_y</th>\n",
       "      <th>tutorial</th>\n",
       "      <th>intermediate_X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-05_06-29-49</td>\n",
       "      <td>0.722</td>\n",
       "      <td>data_clean_y_112394719416_139894881723560.json</td>\n",
       "      <td>tutorial_139894881726304.ipynb</td>\n",
       "      <td>data_clean_X_112394722104_139894881725744.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  utag  score                                  intermediate_y  \\\n",
       "0  2018-10-05_06-29-49  0.722  data_clean_y_112394719416_139894881723560.json   \n",
       "\n",
       "                         tutorial  \\\n",
       "0  tutorial_139894881726304.ipynb   \n",
       "\n",
       "                                   intermediate_X  \n",
       "0  data_clean_X_112394722104_139894881725744.json  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flor.Experiment('flor_intro').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle back up, what would otherwise be destructive, it is not anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context: Introduction & Background\n",
    "\n",
    "In this tutorial, as in many \"real-world\" cases, you'll be joining an in-progress model development effort. Bob, a fellow member of your team, has already attempted two different data-preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This command just tells flor to summarize the various versions of the experiment named 'risecamp_demo'\n",
    "\"\"\"\n",
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the output. The first column, `utag`, lists the different versions of the experiment by name. We can see there are two past versions of the experiment 'risecamp_demo': `first_preproc`, and `second_preproc`. We can also see the artifacts that were involved in the experiment, either as inputs or outputs to some action: `data_loc`, `preprocessing`, `intermediate_X`, and `intermediate_Y`. The name of such artifacts corresponds to the name of the columns in the summary dataframe, we will discuss the values of the cells later in the tutorial. Although we now know there were four (4) artifacts involved in each past version of the experiment, from the summary alone, we cannot (in general) infer how an artifact was produced or consumed in an experiment -- that is, we cannot infer the structure of the experiment computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect the structure of the experiment computation graph, for each of the two previous versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').plot('first_preproc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').plot('second_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both node-link diagrams look the same. This means that the structure of the different experiment versions is the same; however, it is very likely that the contents of the computation graph differ. To see where the difference is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').diff('first_preproc', 'second_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the name `preprocessing` appears in a rectangle and in an ellipse: this means that `preprocessing` is an artifact and an action. This is how we know Flor is tracking the code (e.g. `something.py`), as well as the _execution_ of the code. As in _Make_, the code itself is a dependency to its execution. Next, we see that `data_loc` is an input to `preprocessing`, and `preprocessing` outputs two artifacts: `intermediate_X` and `intermediate_y`.\n",
    "\n",
    "From this information, we learn that Bob tried two (probably different) preprocessing methods on the source data, and split the preprocessed data into a training matrix, and labels. We can continue to audit Bob with Flor, and this would alone be an interesting and worthwhile activity, but for the purposes of this tutorial, we will start by _using_ the preprocessed data created by Bob, and inspect it only if we need to.\n",
    "\n",
    "Now, we're ready for the hands-on portion of the tutorial, where we will be training models with Bob's data to predict the polarity (Good, Bad) of English phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Training a model on each version of the data, followed by data inspection for each version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we brought ourselves \"up to speed\" with the preprocessing work that our colleague Bob had undertaken. We learned the `utag` of the past runs (or previous versions), and this name gave us a 'handle' we can use to get a hold of the artifacts that existed or were derived in the past. This means we can use the `intermediate_X` and `intermediate_y` artifacts from the past, rather than having to re-derive them for our activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a reminder of what the previous experiment versions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from florfunctions import traintest_split, train_test\n",
    "\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag=\"first_preproc\")\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag=\"first_preproc\")\n",
    "    \n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    \n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "    \n",
    "    hyperparameter = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "    \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, hyperparameter])\n",
    "    \n",
    "    report = ex.artifact('report.csv', 'report', do_test)\n",
    "    score = ex.literal(name='score', parent=do_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull\n",
    "\n",
    "Finally, we are ready to run the experiment! We can do so by running `pull()` on our output artifacts. Before doing this, however, it is helpful to use `plot()` to generate a florplan, a graph representation of the artifact's lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "score.pull('first_pull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull again, trying a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.version = \"second_preproc\"\n",
    "data_y.version = \"second_preproc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.pull('second_pull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    report = ex.artifact('report.csv', 'report', utag='first_pull')\n",
    "report.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    report = ex.artifact('report.csv', 'report', utag='second_pull')\n",
    "report.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the data: since it's the only thing that we changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    data_x1 = ex.artifact('data_clean_X.json', 'intermediate_X', utag=\"first_preproc\")\n",
    "    data_x2 = ex.artifact('data_clean_X.json', 'intermediate_X', utag=\"second_preproc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x1.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x2.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Flor makes it convenient to run models using different hyperparameters and track the results. In the `train_test` we created function, notice that we pass in `hyperparameters` in addition to the train and test data. These hyperparameters will allow us to tune our model and track results with ease; let's define them in our experiment setup.\n",
    "\n",
    "Notice that the Random Forest Classifier contains `n_estimators` as a hyperparameter. We would like to tune this hyperparameter and track model performance. In order to specify the hyperparameters, we must make a `literalForEach` within our experiment. **Fill in the `literalForEach` with values 5, 50 and 75 within the experiment below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: We have copied the same experiment below for convenience.\n",
    "#We can also create flor experiments using a context manager.\n",
    "\n",
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "\n",
    "    # Defines artifacts\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='second')\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='second')\n",
    "\n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "\n",
    "    #Define the hyperparameters for the models\n",
    "    random_forest_Nestimators = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "#     random_forest_Nestimators = ex.literalForEach(v=[5, 10, 15], name=\"hyperparameters\", default=5) #SOLUTION\n",
    "\n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    report = ex.artifact('report.csv', 'report', do_test)\n",
    "    score = ex.literal(name='score', parent=do_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "score.pull(utag=\"hyperparameter_tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Results\n",
    "\n",
    "After running the model with different hyperparameters above, we are able to peek at our output artifact, containing precision and recall metrics for the different models. Run the following cell - **which hyperparameter yields the best model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flor\n",
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Better Model\n",
    "\n",
    "Now that you have some experience using flor, let's try using a different model to see if we can improve the results. Some of the classifiers we recommend trying are the Multilayer Perceptron Classifier, Naive Bayes Classifier, and K-neighbors Classifier.\n",
    "\n",
    "After implementing your model of choice in the `train_test` function in **florfunctions.py**, run the cells below to reimport the function and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from florfunctions import train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: We have copied the same experiment below for convenience.\n",
    "#We can also create flor experiments using a context manager.\n",
    "\n",
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    # Defines artifacts\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='first')\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='first')\n",
    "\n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "\n",
    "    #Define the hyperparameters for the models\n",
    "    #hyperparameter = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "    random_forest_Nestimators = ex.literalForEach(v=[5, 10, 15], name=\"hyperparameters\", default=5) #SOLUTION\n",
    "    #MLP_hidden_layer_size = ex.literalForEach(v=[(1, ), (2, ), (3, )], name=\"hyperparameters\", default=(2, ))\n",
    "\n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    #change to MLP_hidden_layer_size \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    #for naive bayes\n",
    "    #do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    report = ex.artifact('report.csv', 'report', do_test)\n",
    "    score = ex.literal(name='score', parent=do_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score.pull(utag=\"improved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Of all the things we tried, what helped the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize().cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
