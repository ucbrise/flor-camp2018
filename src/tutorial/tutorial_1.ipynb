{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flor\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![object model](./object_model.png)\n",
    "<center><i>Artifacts in rectangles; Literals in underline; Actions in circles</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment before starting the activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by importing Flor and letting it know the name of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flor\n",
    "import flor\n",
    "\n",
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial_1.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll go through a short exercise to illustrate a common interactive model-training pattern. Here, we hope to demonstrate the value of versioning and provenance to motivate Flor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. We're going to train a model to predict the sentiment polarity (positive, or negative) of English phrases. The model-training pipeline is fairly standard: we load a dataset, do some light preprocessing, do a train/test split, train the model, and then validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Data\n",
    "movie_reviews = pd.read_json('data.json')\n",
    "\n",
    "# Do light preprocessing\n",
    "movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "# Do train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                          test_size=0.20, random_state=92)\n",
    "\n",
    "# Vectorize the English sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_tr)\n",
    "X_tr = vectorizer.transform(X_tr)\n",
    "X_te = vectorizer.transform(X_te)\n",
    "\n",
    "# Fit the model              ##############\n",
    "clf = RandomForestClassifier(n_estimators=5).fit(X_tr, y_tr)\n",
    "                             ##############\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "# Validate the predictions\n",
    "c = classification_report(y_te, y_pred)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a report above. The precision and recall are hovering around 70%. From reading the code block, we also noticed that the Random Forest classifier was trained on just **5 estimators**, let's see what happens if we **increase the number of estimators to 7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a new model            ###############\n",
    "clf2 = RandomForestClassifier(n_estimators=7).fit(X_tr, y_tr)\n",
    "                             ###############\n",
    "y_pred = clf2.predict(X_te)\n",
    "\n",
    "# Validate the predictions\n",
    "score = clf2.score(X_te, y_te)\n",
    "c = classification_report(y_te, y_pred)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a minor improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise we just completed, we enforced some rudimentary forms of versioning and provenance. For versioning, rather than over-writing our model-fitting cell, we created a new cell, and rather than overwriting the `clf` variable, we created a new `clf2` variable. Moreover, if we save and version this notebook in a version control system such as Git, we might be able to restore the current version of this notebook, and fit a very similar model to the one we wanted. Given these practices, if we break something, we can rollback. As for provenance, the plaintext accompanying the cells (as well as the in-line comments) will allow a human to interpret causal effects: the only thing we changed was the number of estimators, and we changed the number of estimators from 5 to 7, which improved the quality metrics of the model. This human analyst would be able to answer questions such as, what data did you read, and where did you read it from? And, what model did you fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite some of these benefits, this rudimentary form of versioning and provenance tracking doesn't scale well. An analyst rushing to meet a deadline may over-write the same cell rather than make a copy, or forget to commit to Git after any change to the notebook: this would compromise our quality of versioning. Moreover, it's easy to see that as the complexity of the model-training pipeline increases from a few dozen lines of code to the thousands, and the model-training pipeline involves some distribution of code and data, and use of heterogeneous and complex infrastructure, manual human inspection of source code will be an inadequate substitute for a provenance management system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis in Flor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flor is a context-first system for managing the pipeline development phase of the ML Lifecycle. Two of the many useful things Flor supports is automatic versioning of experiments and provenance tracking. We'll start by showing these two features of Flor, while emphasizing how little effort it takes to wrap a pre-existing pipeline (namely, the one we ran before) in Flor. Here, we hope to convince you that you would rather use Flor than implement versioning and provenance tracking yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN FLOR EXPERIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we've copied the model-training pipeline (with 5 estimators). We highlight the differences in-line using `###`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "@flor.track_action('risecamp_demo')\n",
    "def split_train_and_eval(**kwargs):\n",
    "###################################\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    movie_reviews = pd.read_json('data.json')\n",
    "    movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                              test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    clf = RandomForestClassifier(n_estimators=7).fit(X_tr, y_tr)\n",
    "    \n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    c = classification_report(y_te, y_pred)\n",
    "\n",
    "    print(c)\n",
    "    \n",
    "    #######################\n",
    "    return {'score': score}\n",
    "    #######################\n",
    "split_train_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, and as expected, we see that the score is hovering around 70%. Moreover, we see the dataflow graph of our `risecamp_demo` experiment. We read it as follows: the function `split_train_and_eval` defined in the tutorial artifact (`tutorial.ipynb`) is run to produce a `score` value. Despite its simplicity and coarse granularity, this is a provenance graph for the `score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to make 2 changes to wrap our model-training pipeline in Flor:\n",
    "1. Copy and paste the code into a decorated function\n",
    "2. Return the value we want Flor to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NOTE THIS IS FIRST TIME THEY SEE DF] But we also see that our Jupyter notebook was automatically versioned (see the cell in the `tutorial` columns), and all of the relevant artifacts were associated with the execution name `first_pull`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important instructions*** Now, you will jump to **BEGIN FLOR EXPERIMENT** (scroll up), and modify the cell underneath that label. Change the number of estimators from 5 to 7 (Line 21) and re-run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you should see that what would have been a destructive change without Flor (over-writing a cell and re-running it), is properly handled with Flor. Flor automatically preserves versions and tracks provenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finer granularity tracking in Flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.track_action('risecamp_demo')\n",
    "                         ############  #########\n",
    "def split_train_and_eval(n_estimators, max_depth, **kwargs):\n",
    "                         ############  #########\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    movie_reviews = pd.read_json('data.json')\n",
    "    movie_reviews['rating'] = movie_reviews['rating'].map(lambda x: 0 if x < 5 else 1)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(movie_reviews['text'], movie_reviews['rating'], \n",
    "                                              test_size=0.20, random_state=92)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)         ############            #########\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth).fit(X_tr, y_tr)\n",
    "                                              ############            #########\n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    score = clf.score(X_te, y_te)\n",
    "    c = classification_report(y_te, y_pred)\n",
    "\n",
    "    print(c)\n",
    "    \n",
    "    #######################\n",
    "    return {'score': score}\n",
    "    #######################\n",
    "    \n",
    "split_train_and_eval(7, [10, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
