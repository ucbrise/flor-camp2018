{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Finer-Grained Annotation and Versioning\n",
    "\n",
    "Now that we've learned how to use Flor's expressive syntax, we will annotate and version the experiment we have been developing to capture more detailed context about the inputs, code and outputs.\n",
    "\n",
    "Goals of this notebook:\n",
    "- Factor our monolithic training script into 3 encapsulated Flor Actions, which pass Artifacts and Literals between them that can be tracked by Flor\n",
    "- Connect our Actions and Artifacts into a *Flor Plan* workflow DAG\n",
    "- See how previously derived Artifacts can be reused without rerunning monolithic scripts\n",
    "- Query Flor's automatically-tracked context to identify and serve our best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your environment before starting the activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we're going to start by importing Flor and letting it know the name of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Flor\n",
    "import flor\n",
    "\n",
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial_3.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll provide Flor annotations for all our code in advance\n",
    "\n",
    "Before we can put together a multi-step experiment in Flor, we need to define the individual Flor functions for each step. The next 3 code cells correspond to 3 Flor actions that we will compose into a *Flor plan* below.\n",
    "\n",
    "Our first action, shown in the next cell, takes care of splitting the data. `split` is our first \"encapsulated\" Flor function, and it's useful to spend some time understanding its arguments and how they are used. When we compose `split` in our Flor plan below, we'll see that:\n",
    "- `intermediate_X` and `intermediate_y` are input Flor Artifacts -- references to files (of data). The `split` routine makes standard Python I/O calls on these (lines 7-14, 19-27) as it would with any file handles, but Flor is able to track the versions of these files during each experiment run.\n",
    "- `test_size` and `random_state` are input Flor Literals -- passed by value. Note that both of these Literals are passed in the subroutine call to `train_test_split` in Line 17; Flor will be able to track the versions of these Literals as they pass through code during an experiment run. \n",
    "- `X_train`, `X_test`, `y_train` and `y_test` are Flor Artifacts that represent the *output* of this function; again they will be tracked by Flor in each experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def split(intermediate_X, intermediate_y, test_size, random_state, \n",
    "          X_train, X_test, y_train, y_test, **kwargs):\n",
    "    import json\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "        \n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "        \n",
    "    # Split the data\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Write the outputs\n",
    "    with open(X_train, 'w') as f:\n",
    "        json.dump(X_tr, f)\n",
    "    with open(X_test, 'w') as f:\n",
    "        json.dump(X_te, f)\n",
    "    with open(y_train, 'w') as f:\n",
    "        json.dump(y_tr, f)\n",
    "    with open(y_test, 'w') as f:\n",
    "        json.dump(y_te, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, notice the input parameters include `X_train` and `y_train`, which we will use to pass in the output Artifacts from the previous cell's `split` function. `model` and `vectorizer` are output Artifacts of the `train` function below; they are written into files in Lines 26-30, containing serialized Python objects that will also be tracked by Flor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train(X_train, y_train, n_estimators, max_depth, model, vectorizer, **kwargs):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import cloudpickle\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(X_train, 'r') as f:\n",
    "        X_tr = json.load(f)\n",
    "    with open(y_train, 'r') as f:\n",
    "        y_tr = json.load(f)\n",
    "    \n",
    "    # Fit the vectorizer\n",
    "    vec = TfidfVectorizer()\n",
    "    vec.fit(X_tr)\n",
    "    \n",
    "    # Transform the training data\n",
    "    X_tr = vec.transform(X_tr)\n",
    "    \n",
    "    # Train the model\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth).fit(X_tr, y_tr)\n",
    "    \n",
    "    # Write the output\n",
    "    with open(model, 'wb') as f:\n",
    "        cloudpickle.dump(clf, f)\n",
    "    with open(vectorizer, 'wb') as f:\n",
    "        cloudpickle.dump(vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `eval` function below takes as arguments the test data, model, and vectorizer Artifacts and evaluates the model we just trained. Note line 24: the `return` of a Flor func is a Flor Literal that is tracked in each experiment run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def eval(X_test, y_test, model, vectorizer, **kwargs):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import cloudpickle\n",
    "    \n",
    "    # Read the inputs\n",
    "    with open(X_test, 'r') as f:\n",
    "        X_te = json.load(f)\n",
    "    with open(y_test, 'r') as f:\n",
    "        y_te = json.load(f)\n",
    "    with open(model, 'rb') as f:\n",
    "        clf = cloudpickle.load(f)\n",
    "    with open(vectorizer, 'rb') as f:\n",
    "        vec = cloudpickle.load(f)\n",
    "    \n",
    "    # Test the model\n",
    "    X_te = vec.transform(X_te)\n",
    "    score = clf.score(X_te, y_te)\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    # Return the score\n",
    "    return {'score': score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's build a detailed Flor Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written the code, and we know which resources to read (Bob's latest preprocessed data), we can specify the Flor Plan, exposing all hyper-parameters and intermediate Artifacts. \n",
    "\n",
    "Look at lines 13-19; they wire the `split` Action into the Flor plan with its inputs and outputs. The inputs are declared in the `in_artifacts` parameter to `ex.action` in line 13. Each of lines 14-17 specifies an output Artifact; we connect that Artifact to a specific argument of the `split` Action by (a) referencing `do_split` in the `parent` argument to `ex.artifact`, and (b) declaring the desired argument of `split` via the `name` argument of `ex.artifact`.\n",
    "\n",
    "A similar pattern wires in `train` in lines 19-21, and `eval` in lines 23-24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with flor.Experiment('bob_preproc') as bob, flor.Experiment('risecamp_demo') as ex:\n",
    "    # This is how we tell Flor we will be using Bob's derived artifacts\n",
    "    data_x = bob.artifact('data_clean_X.json', 'intermediate_X', label=\"second_preproc\")\n",
    "    data_y = bob.artifact('data_clean_y.json', 'intermediate_y', label=\"second_preproc\")\n",
    "    \n",
    "    # Here we declare all the static literals\n",
    "    random_state = ex.literal(92, 'random_state')\n",
    "    test_size = ex.literal(0.20, 'test_size')\n",
    "    n_estimators = ex.literal(7, 'n_estimators') \n",
    "    max_depth = ex.literal(100, 'max_depth')\n",
    "    \n",
    "    # Now we connect the Flor Plan\n",
    "    do_split = ex.action(func=split, in_artifacts=[data_x, data_y, test_size, random_state])\n",
    "    X_train = ex.artifact(loc='X_train.json', name='X_train', parent=do_split)\n",
    "    X_test = ex.artifact(loc='X_test.json', name='X_test', parent=do_split)\n",
    "    y_train = ex.artifact(loc='y_train.json', name='y_train', parent=do_split)\n",
    "    y_test = ex.artifact(loc='y_test.json', name='y_test', parent=do_split)\n",
    "    \n",
    "    do_train = ex.action(func=train, in_artifacts=[X_train, y_train, n_estimators, max_depth])\n",
    "    model = ex.artifact(loc='model.pkl', name='model', parent=do_train)\n",
    "    vectorizer = ex.artifact(loc='vectorizer.pkl', name='vectorizer', parent=do_train)\n",
    "    \n",
    "    do_eval = ex.action(func=eval, in_artifacts=[X_test, y_test, model, vectorizer])\n",
    "    score = ex.literal(name='score', parent=do_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to this point, we only saw Flor Plans with a single action. Now we can call `score.plot()` to see our more involved Flor Plan.  In the plot generated by the next cell, rectangles correspond to Artifacts, which the user is responsible for reading/writing, but which Flor automatically tracks and versions. The underlined names correspond to literals, which Flor both stores and tracks across runs. \n",
    "\n",
    "The distinction between Artifacts and Literals can sometimes feel arbitrary. As a rule of thumb, if an object is relatively small (in bytes) and can be serialized as a string without losing information, then it can be a Literal. Otherwise, it's best to represent it as an Artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pull the score to execute the experiment we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score.pull('seventh_pull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the score is still hovering around 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flor.Experiment('risecamp_demo').summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the finer-grained annotations resulted in more detailed information about our experimental pipeline. The Flor plan itself is a more helpful description of the high-level intent of the code, and serves as a form of documentation. Also, because Flor is tracking intermediate Artifacts and Literals, the `summarize` call is able to provide more information about what is computed and stored along the way. These intermediate Artifacts and Literals are automatically versioned by Flor, and can be individually examined and `diff`ed across runs for more granular tracking of changes across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing models from previous Flor experiments\n",
    "Another benefit of exposing the intermediate Artifacts and Literals between smaller Actions is that previously-computer intermediate objects can be reused without requiring recomputation. In the next cells we will reuse the output Artifacts from our earlier runs, examine them, and \"serve\" them in a toy application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout best model and serve it\n",
    "Here, we're going to retrieve the model and vectorizer we just fitted with Flor. Lines 1-3 of the next cell declare a Flor experiment in the expressive syntax. In lines 4-5, rather than calling the Flor `pull` method (which runs an experiment and stores the output) we use Flor's `peek` method to build each Artifact and load it into a Python variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    model = ex.artifact('model.pkl', 'model', label='seventh_pull')\n",
    "    vectorizer = ex.artifact('vectorizer.pkl', 'vectorizer', label='seventh_pull')\n",
    "model = model.peek()\n",
    "vec = vectorizer.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the model into a toy application that tries to predict the sentiment from your phrases. Try out some phrases and see how the model does!  Enter `nothing` to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROMPT = \"What's on your mind? \"\n",
    "\n",
    "phrase = input(PROMPT)\n",
    "while phrase[0:len('nothing')].lower() != 'nothing':\n",
    "    phrase = vec.transform([phrase,])\n",
    "    positive = model.predict(phrase)\n",
    "    if positive:\n",
    "        print('Happy to hear that!\\n'.format(phrase))\n",
    "    else:\n",
    "        print(\"Sorry about that...\\n\")\n",
    "    phrase = input(PROMPT)\n",
    "print('you said nothing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "To summarize our three notebooks, we've been reminded of the value of annotating and versioning experiments, and witnessed how Flor can automate these responsibilities with as few as 3 lines of code. Moreover, when finer-grained tracking is desired, Flor is able to capture more meta-data and store derived artifacts for future retrieval. Flor enables experiment interpretation and re-using artifacts across experiments."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
