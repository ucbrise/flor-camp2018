{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi: Predict Trip Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a pipeline that predicts the trip duration from attributes such as pickup location, distance traveled, time of day, and so on. This pipeline is a typical ML workflow written in Python, using Scikit-learn (RandomForrest). We see pre-processing, training, and testing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.7793251793702737\n",
      "RMSE: 303.39793188582206\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/stephaniestallworth/nyc-taxi-eda-regression-fivethirtyeight-viz/notebook\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "def roundtime(tstring):\n",
    "    hours, mins, secs = tstring.split(':')\n",
    "    if int(mins) >= 30:\n",
    "        if hours == '23':\n",
    "            return '00'\n",
    "        else:\n",
    "            return str(int(hours) + 1)\n",
    "    else:\n",
    "        return hours\n",
    "\n",
    "def weekday(start):\n",
    "    from datetime import datetime\n",
    "    fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    tstamp = datetime.strptime(start, fmt)\n",
    "    return int(tstamp.weekday())\n",
    "\n",
    "data_df = pd.read_csv('train.csv')\n",
    "\n",
    "data_df['distance'] = [i for i in map(manhattan_distance,\n",
    "    data_df['pickup_longitude'], data_df['pickup_latitude'], \n",
    "    data_df['dropoff_longitude'], data_df['dropoff_latitude'])]\n",
    "\n",
    "# Remove outliers in passenger_count\n",
    "data_df = data_df[data_df['passenger_count']>0]\n",
    "data_df = data_df[data_df['passenger_count']<9]\n",
    "\n",
    "# Remove coordinate outliers\n",
    "data_df = data_df[data_df['pickup_longitude'] <= -73.75]\n",
    "data_df = data_df[data_df['pickup_longitude'] >= -74.03]\n",
    "data_df = data_df[data_df['pickup_latitude'] <= 40.85]\n",
    "data_df = data_df[data_df['pickup_latitude'] >= 40.63]\n",
    "data_df = data_df[data_df['dropoff_longitude'] <= -73.75]\n",
    "data_df = data_df[data_df['dropoff_longitude'] >= -74.03]\n",
    "data_df = data_df[data_df['dropoff_latitude'] <= 40.85]\n",
    "data_df = data_df[data_df['dropoff_latitude'] >= 40.63]\n",
    "\n",
    "# Remove trip_duration outliers\n",
    "trip_duration_mean = np.mean(data_df['trip_duration'])\n",
    "trip_duration_std = np.std(data_df['trip_duration'])\n",
    "data_df = data_df[data_df['trip_duration'] <= trip_duration_mean + 2*trip_duration_std]\n",
    "data_df = data_df[data_df['trip_duration'] >= trip_duration_mean - 2*trip_duration_std]\n",
    "data_df = data_df[data_df['trip_duration'] >= 30]\n",
    "data_df = data_df[data_df['trip_duration'] <= 60*240]\n",
    "\n",
    "data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "X = data_df[['vendor_id', 'pickup_longitude',\n",
    "            'pickup_latitude', 'dropoff_longitude', \n",
    "            'dropoff_latitude', 'distance',\n",
    "            'start_hr', 'start_month', 'start_weekday']]\n",
    "y = data_df['trip_duration']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "    y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=20, n_jobs=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "score = metrics.explained_variance_score(y_test, preds)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
    "\n",
    "print(\"R2: {}\".format(score))\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarsest Flor Pipeline: Wrap an existing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect many of our users to already have pipelines that they built and refined over time. We expect our users to trust these pipelines, and these pipelines to be effective. For such users, we add value by versioning the artifacts the user wants us to track, as well as recording the relationships between some objects at a corase level. To wrap an existing pipeline in Flor, simply put the pipeline code inside a decorated function, and tell us what the inputs and outputs of the function are. Please see example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flor\n",
    "\n",
    "# Create a context manager for the experiment and is named 'coarsest'\n",
    "with flor.Experiment('coarsest') as ex:\n",
    "    \n",
    "    #If the notebook name has not already been set, you are able to set the name in code. \n",
    "    flor.setNotebookName('Taxi.ipynb')\n",
    "    \n",
    "    #Initialize the ground client for a particular experiment\n",
    "    ex.groundClient('ground')\n",
    "    \n",
    "    #Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "    @flor.func\n",
    "    def run_existing_pipeline(path_to_data):\n",
    "        import math\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn import metrics\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        from sklearn.model_selection import train_test_split\n",
    "    \n",
    "        def manhattan_distance(x1, y1, x2, y2):\n",
    "            return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "        def roundtime(tstring):\n",
    "            hours, mins, secs = tstring.split(':')\n",
    "            if int(mins) >= 30:\n",
    "                if hours == '23':\n",
    "                    return '00'\n",
    "                else:\n",
    "                    return str(int(hours) + 1)\n",
    "            else:\n",
    "                return hours\n",
    "\n",
    "        def weekday(start):\n",
    "            from datetime import datetime\n",
    "            fmt = '%Y-%m-%d %H:%M:%S'\n",
    "            tstamp = datetime.strptime(start, fmt)\n",
    "            return int(tstamp.weekday())\n",
    "\n",
    "        data_df = pd.read_csv(path_to_data)\n",
    "\n",
    "        data_df['distance'] = [i for i in map(manhattan_distance,\n",
    "            data_df['pickup_longitude'], data_df['pickup_latitude'], \n",
    "            data_df['dropoff_longitude'], data_df['dropoff_latitude'])]\n",
    "\n",
    "        # Remove outliers in passenger_count\n",
    "        data_df = data_df[data_df['passenger_count']>0]\n",
    "        data_df = data_df[data_df['passenger_count']<9]\n",
    "\n",
    "        # Remove coordinate outliers\n",
    "        data_df = data_df[data_df['pickup_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['pickup_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['pickup_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['pickup_latitude'] >= 40.63]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] >= 40.63]\n",
    "\n",
    "        # Remove trip_duration outliers\n",
    "        trip_duration_mean = np.mean(data_df['trip_duration'])\n",
    "        trip_duration_std = np.std(data_df['trip_duration'])\n",
    "        data_df = data_df[data_df['trip_duration'] <= trip_duration_mean + 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= trip_duration_mean - 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= 30]\n",
    "        data_df = data_df[data_df['trip_duration'] <= 60*240]\n",
    "\n",
    "        data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "        data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "        data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "        X = data_df[['vendor_id', 'pickup_longitude',\n",
    "                    'pickup_latitude', 'dropoff_longitude', \n",
    "                    'dropoff_latitude', 'distance',\n",
    "                    'start_hr', 'start_month', 'start_weekday']]\n",
    "        y = data_df['trip_duration']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "            y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "        clf = RandomForestRegressor(n_estimators=20, n_jobs=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        preds = clf.predict(X_test)\n",
    "        score = metrics.explained_variance_score(y_test, preds)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
    "\n",
    "        score = \"R2: {}\".format(score)\n",
    "        rmse = \"RMSE: {}\".format(rmse)\n",
    "        \n",
    "        print(score, rmse)\n",
    "    \n",
    "        return clf, score, rmse\n",
    "    \n",
    "    #Create a Flor artifact that corresponds to the input 'train.csv'\n",
    "    data = ex.artifact('train.csv')\n",
    "    \n",
    "    #Create a Flor action that maps the input parameters of 'data' to the function 'run_existing_pipeline'\n",
    "    do_all = ex.action(run_existing_pipeline, [data])\n",
    "    \n",
    "    #Create multiple Flor artifacts that correspond to the outputs of the function specified in 'do_all' \n",
    "    model = ex.artifact('model.pkl', do_all)\n",
    "    score = ex.artifact('score.txt', do_all)\n",
    "    rmse = ex.artifact('rmse.txt', do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<flor.viz.VizNode object at 0x7f97bc1f5588>, <flor.viz.VizNode object at 0x7f97bc1f5a90>, <flor.viz.VizNode object at 0x7f97bc1f5860>]\n"
     ]
    }
   ],
   "source": [
    "#plot() creates a graph representation of the lineage to a particular artifact\n",
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.7798383654104799 RMSE: 303.04352147995013\n",
      "{<flor.object_model.artifact.Artifact object at 0x7fc85fc8bc18>, <flor.object_model.artifact.Artifact object at 0x7fc85fc7f438>, <flor.object_model.artifact.Artifact object at 0x7fc85fc8bc50>, <flor.object_model.action.Action object at 0x7fc85fc7f320>}\n",
      "score.txt\n",
      "model.pkl\n",
      "rmse.txt\n"
     ]
    }
   ],
   "source": [
    "score.pull()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Flor Pipeline with Hyper-parameter Sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe after some time, a trusted and effective pipeline may need some tuning. We enable pipeline (and model) tuning via Flor Literals. A user can declare a Literal, with some iterable value or any Python basic type. When a literal is passed to an Action, the experiment may become a multi-trial experiment. Flor is able to execute every trial with some optimizations, and relate the experiment outcomes to the experiment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flor\n",
    "\n",
    "# Create a context manager for the experiment and is named 'coarsest'\n",
    "with flor.Experiment('coarse') as ex:\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #If the notebook name has not already been set, you are able to set the name in code. \n",
    "    flor.setNotebookName('Taxi.ipynb')\n",
    "    \n",
    "    #Initialize the ground client for a particular experiment\n",
    "    ex.groundClient('ground')\n",
    "    \n",
    "    #Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "    #The decorated function now has an additional parmeter of 'n_estimators'\n",
    "    @flor.func\n",
    "    def run_existing_pipeline(path_to_data, n_estimators):\n",
    "    \n",
    "        def manhattan_distance(x1, y1, x2, y2):\n",
    "            return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "        def roundtime(tstring):\n",
    "            hours, mins, secs = tstring.split(':')\n",
    "            if int(mins) >= 30:\n",
    "                if hours == '23':\n",
    "                    return '00'\n",
    "                else:\n",
    "                    return str(int(hours) + 1)\n",
    "            else:\n",
    "                return hours\n",
    "\n",
    "        def weekday(start):\n",
    "            from datetime import datetime\n",
    "            fmt = '%Y-%m-%d %H:%M:%S'\n",
    "            tstamp = datetime.strptime(start, fmt)\n",
    "            return int(tstamp.weekday())\n",
    "\n",
    "        data_df = pd.read_csv(path_to_data)\n",
    "\n",
    "        data_df['distance'] = [i for i in map(manhattan_distance,\n",
    "            data_df['pickup_longitude'], data_df['pickup_latitude'], \n",
    "            data_df['dropoff_longitude'], data_df['dropoff_latitude'])]\n",
    "\n",
    "        # Remove outliers in passenger_count\n",
    "        data_df = data_df[data_df['passenger_count']>0]\n",
    "        data_df = data_df[data_df['passenger_count']<9]\n",
    "\n",
    "        # Remove coordinate outliers\n",
    "        data_df = data_df[data_df['pickup_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['pickup_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['pickup_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['pickup_latitude'] >= 40.63]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] >= 40.63]\n",
    "\n",
    "        # Remove trip_duration outliers\n",
    "        trip_duration_mean = np.mean(data_df['trip_duration'])\n",
    "        trip_duration_std = np.std(data_df['trip_duration'])\n",
    "        data_df = data_df[data_df['trip_duration'] <= trip_duration_mean + 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= trip_duration_mean - 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= 30]\n",
    "        data_df = data_df[data_df['trip_duration'] <= 60*240]\n",
    "\n",
    "        data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "        data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "        data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "        X = data_df[['vendor_id', 'pickup_longitude',\n",
    "                    'pickup_latitude', 'dropoff_longitude', \n",
    "                    'dropoff_latitude', 'distance',\n",
    "                    'start_hr', 'start_month', 'start_weekday']]\n",
    "        y = data_df['trip_duration']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "            y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "        clf = RandomForestRegressor(n_estimators=n_estimators, n_jobs=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        preds = clf.predict(X_test)\n",
    "        score = metrics.explained_variance_score(y_test, preds)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
    "\n",
    "        score = \"R2: {}\".format(score)\n",
    "        rmse = \"RMSE: {}\".format(rmse)\n",
    "        \n",
    "        print(score, rmse)\n",
    "    \n",
    "        return clf, score, rmse\n",
    "    \n",
    "    #Create a Flor artifact that corresponds to the input 'train.csv'\n",
    "    data = ex.artifact('train.csv')\n",
    "    \n",
    "    #Create a Flor literal that corresponds to the array specified by the first parameter\n",
    "    num_est = ex.literal([15, 20, 30], 'num_estimators')\n",
    "    \n",
    "    #Changes the configuration of the Flor literal to be evaluated element-wise of the array specified above\n",
    "#     num_est.forEach()\n",
    "    \n",
    "    #Create a Flor action that maps the input parameters of 'data' and 'num_est' \n",
    "    #to the function 'run_existing_pipeline'\n",
    "    do_all = ex.action(run_existing_pipeline, [data, num_est])\n",
    "    \n",
    "    #Create multiple Flor artifacts that correspond to the outputs of the function specified in 'do_all' \n",
    "    model = ex.artifact('model.pkl', do_all)\n",
    "    score = ex.artifact('score.txt', do_all)\n",
    "    rmse = ex.artifact('rmse.txt', do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot() creates a graph representation of the lineage to a particular artifact\n",
    "score.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Flor Pipeline: Reuse Intermediate Artifacts & Compose Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps our user will observe that the preprocessing step produces a result that can be shared with the entire organization; alternatively, it's possible that the pre-processing step is expensive, and the user will not want to re-run this step every time that the system runs the pipeline with a new configuration (caching intermediate shared results). A subgraph in the Flor pipeline may be its own experiment, or part of a parent experiment. When the Flor pipeline is shared with others (as opposed to merely sharing the end results), it's possible for other users or customers of the data to understand the lineage of the results: what transformations were applied, and what the source was, etc. We include an example below for how a user might separete the preprocessing step from the remaining pipeline, so that the subpipeline may be more easily shared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import flor\n",
    "\n",
    "# Create a context manager for the experiment and is named 'fine'\n",
    "with flor.Experiment('fine') as ex:\n",
    "    \n",
    "    #If the notebook name has not already been set, you are able to set the name in code. \n",
    "    flor.setNotebookName('Taxi.ipynb')\n",
    "    \n",
    "    #Initialize the ground client for a particular experiment\n",
    "    ex.groundClient('ground')\n",
    "    \n",
    "    #Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "    @flor.func\n",
    "    def prepare_data(path_to_data):\n",
    "    \n",
    "        def manhattan_distance(x1, y1, x2, y2):\n",
    "            return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "        def roundtime(tstring):\n",
    "            hours, mins, secs = tstring.split(':')\n",
    "            if int(mins) >= 30:\n",
    "                if hours == '23':\n",
    "                    return '00'\n",
    "                else:\n",
    "                    return str(int(hours) + 1)\n",
    "            else:\n",
    "                return hours\n",
    "\n",
    "        def weekday(start):\n",
    "            from datetime import datetime\n",
    "            fmt = '%Y-%m-%d %H:%M:%S'\n",
    "            tstamp = datetime.strptime(start, fmt)\n",
    "            return int(tstamp.weekday())\n",
    "\n",
    "        data_df = pd.read_csv(path_to_data)\n",
    "\n",
    "        data_df['distance'] = [i for i in map(manhattan_distance,\n",
    "            data_df['pickup_longitude'], data_df['pickup_latitude'], \n",
    "            data_df['dropoff_longitude'], data_df['dropoff_latitude'])]\n",
    "\n",
    "        # Remove outliers in passenger_count\n",
    "        data_df = data_df[data_df['passenger_count']>0]\n",
    "        data_df = data_df[data_df['passenger_count']<9]\n",
    "\n",
    "        # Remove coordinate outliers\n",
    "        data_df = data_df[data_df['pickup_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['pickup_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['pickup_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['pickup_latitude'] >= 40.63]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] <= -73.75]\n",
    "        data_df = data_df[data_df['dropoff_longitude'] >= -74.03]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] <= 40.85]\n",
    "        data_df = data_df[data_df['dropoff_latitude'] >= 40.63]\n",
    "\n",
    "        # Remove trip_duration outliers\n",
    "        trip_duration_mean = np.mean(data_df['trip_duration'])\n",
    "        trip_duration_std = np.std(data_df['trip_duration'])\n",
    "        data_df = data_df[data_df['trip_duration'] <= trip_duration_mean + 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= trip_duration_mean - 2*trip_duration_std]\n",
    "        data_df = data_df[data_df['trip_duration'] >= 30]\n",
    "        data_df = data_df[data_df['trip_duration'] <= 60*240]\n",
    "\n",
    "        data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "        data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "        data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "        return data_df\n",
    "    \n",
    "    #Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "    @flor.func\n",
    "    def train_test_model(data_df, n_estimators):\n",
    "        X = data_df[['vendor_id', 'pickup_longitude',\n",
    "                    'pickup_latitude', 'dropoff_longitude', \n",
    "                    'dropoff_latitude', 'distance',\n",
    "                    'start_hr', 'start_month', 'start_weekday']]\n",
    "        y = data_df['trip_duration']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "            y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "        clf = RandomForestRegressor(n_estimators=n_estimators, n_jobs=3)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        preds = clf.predict(X_test)\n",
    "        score = metrics.explained_variance_score(y_test, preds)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
    "\n",
    "        score = \"R2: {}\".format(score)\n",
    "        rmse = \"RMSE: {}\".format(rmse)\n",
    "        \n",
    "        print(score, rmse)\n",
    "    \n",
    "        return clf, score, rmse\n",
    "    \n",
    "    \n",
    "    #Create a Flor artifact that corresponds to the input 'train.csv'\n",
    "    data = ex.artifact('train.csv')\n",
    "    \n",
    "    #Create a Flor literal that corresponds to the array specified by the first parameter\n",
    "    num_est = ex.literal([15, 20, 30], 'num_estimators')\n",
    "    \n",
    "    #Changes the configuration of the Flor literal to be evaluated element-wise of the array specified above\n",
    "    num_est.forEach()\n",
    "    \n",
    "    #Create a Flor action that maps the input parameters of 'data' to the function 'prepare_data'\n",
    "    do_prep = ex.action(prepare_data, [data])\n",
    "    \n",
    "    #Creates a Flor artifact that corresponds to the output of 'do_prep' \n",
    "    #and is stored in the file 'prepped_data.pkl'\n",
    "    prepd = ex.artifact('prepped_data.pkl', do_prep)\n",
    "    \n",
    "    #Create a Flor action that maps the input parameters of 'prepd' and 'num_est' \n",
    "    #to the function 'train_test_model'\n",
    "    do_tr_te = ex.action(train_test_model, [prepd, num_est])\n",
    "\n",
    "    #Create multiple Flor artifacts that correspond to the outputs of the function specified in 'do_tr_te' \n",
    "    model = ex.artifact('model.pkl', do_tr_te)\n",
    "    score = ex.artifact('score.txt', do_tr_te)\n",
    "    rmse = ex.artifact('rmse.txt', do_tr_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot() creates a graph representation of the lineage to a particular artifact\n",
    "score.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finest Pipeline: Flor Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After becoming familiar with Flor, it is likely that our user will want to build her next pipeline using Flor from the start: this was the use-case we envisioed at first. In addition to gaining the benefits from coarser Flor pipelines, the user will be given a set of tools for pipeline development in an interactive environment such as Jupyter. We are currently investigating techniques for detecting poor experiment methods, such as p-hacking or data dredging. Whenever a user \"peeks\" a Flor artifact, the event is recorded. We plan to use this information to infer when information from the test data may have been compromised and contaminated the analysis, and when it's likely that the user has overfitted to the data, and it's time to collect a fresh sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('Taxi.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context manager for the experiment and is named 'finest'\n",
    "ex = flor.Experiment('finest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the ground client for a particular experiment\n",
    "ex.groundClient('ground')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor artifact that corresponds to the input 'train.csv'\n",
    "data = ex.artifact('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def dataframize(csvpath):\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor action that maps the input parameters of 'data' to the function 'dataframize'\n",
    "do_dfize = ex.action(dataframize, [data])\n",
    "\n",
    "#Creates a Flor artifact that corresponds to the output of 'do_dfize' \n",
    "#and is stored in the file 'train_df.pkl'\n",
    "tr_data_df = ex.artifact('train_df.pkl', do_dfize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of *peek*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0801584</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-30 22:01:40</td>\n",
       "      <td>2016-01-30 22:09:03</td>\n",
       "      <td>6</td>\n",
       "      <td>-73.982857</td>\n",
       "      <td>40.742195</td>\n",
       "      <td>-73.992081</td>\n",
       "      <td>40.749184</td>\n",
       "      <td>N</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1813257</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-17 22:34:59</td>\n",
       "      <td>2016-06-17 22:40:40</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.969017</td>\n",
       "      <td>40.757839</td>\n",
       "      <td>-73.957405</td>\n",
       "      <td>40.765896</td>\n",
       "      <td>N</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id1324603</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-21 07:54:58</td>\n",
       "      <td>2016-05-21 08:20:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969276</td>\n",
       "      <td>40.797779</td>\n",
       "      <td>-73.922470</td>\n",
       "      <td>40.760559</td>\n",
       "      <td>N</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id1301050</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-27 23:12:23</td>\n",
       "      <td>2016-05-27 23:16:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.738400</td>\n",
       "      <td>-73.985786</td>\n",
       "      <td>40.732815</td>\n",
       "      <td>N</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id0012891</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 21:45:01</td>\n",
       "      <td>2016-03-10 22:05:26</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981049</td>\n",
       "      <td>40.744339</td>\n",
       "      <td>-73.973000</td>\n",
       "      <td>40.789989</td>\n",
       "      <td>N</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>id1436371</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-10 22:08:41</td>\n",
       "      <td>2016-05-10 22:29:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982651</td>\n",
       "      <td>40.763840</td>\n",
       "      <td>-74.002228</td>\n",
       "      <td>40.732990</td>\n",
       "      <td>N</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>id1299289</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-15 11:16:11</td>\n",
       "      <td>2016-05-15 11:34:59</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.991531</td>\n",
       "      <td>40.749439</td>\n",
       "      <td>-73.956543</td>\n",
       "      <td>40.770630</td>\n",
       "      <td>N</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>id1187965</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-19 09:52:46</td>\n",
       "      <td>2016-02-19 10:11:20</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.962982</td>\n",
       "      <td>40.756680</td>\n",
       "      <td>-73.984406</td>\n",
       "      <td>40.760719</td>\n",
       "      <td>N</td>\n",
       "      <td>1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>id0799785</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-01 20:58:29</td>\n",
       "      <td>2016-06-01 21:02:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.956306</td>\n",
       "      <td>40.767941</td>\n",
       "      <td>-73.966110</td>\n",
       "      <td>40.763000</td>\n",
       "      <td>N</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id2900608</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-27 00:43:36</td>\n",
       "      <td>2016-05-27 01:07:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.992195</td>\n",
       "      <td>40.727226</td>\n",
       "      <td>-73.974655</td>\n",
       "      <td>40.783070</td>\n",
       "      <td>N</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id3319787</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-16 15:29:02</td>\n",
       "      <td>2016-05-16 15:32:33</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.955513</td>\n",
       "      <td>40.768593</td>\n",
       "      <td>-73.948761</td>\n",
       "      <td>40.771545</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>id3379579</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-11 17:29:50</td>\n",
       "      <td>2016-04-11 18:08:26</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991165</td>\n",
       "      <td>40.755562</td>\n",
       "      <td>-73.999290</td>\n",
       "      <td>40.725353</td>\n",
       "      <td>N</td>\n",
       "      <td>2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>id1154431</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-14 08:48:26</td>\n",
       "      <td>2016-04-14 09:00:37</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994255</td>\n",
       "      <td>40.745804</td>\n",
       "      <td>-73.999657</td>\n",
       "      <td>40.723343</td>\n",
       "      <td>N</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>id3552682</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-27 09:55:13</td>\n",
       "      <td>2016-06-27 10:17:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.003983</td>\n",
       "      <td>40.713013</td>\n",
       "      <td>-73.979195</td>\n",
       "      <td>40.749924</td>\n",
       "      <td>N</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>id3390316</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-05 13:47:23</td>\n",
       "      <td>2016-06-05 13:51:34</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.983887</td>\n",
       "      <td>40.738197</td>\n",
       "      <td>-73.991203</td>\n",
       "      <td>40.727871</td>\n",
       "      <td>N</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id2070428</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-02-28 02:23:02</td>\n",
       "      <td>2016-02-28 02:31:08</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980370</td>\n",
       "      <td>40.742420</td>\n",
       "      <td>-73.962852</td>\n",
       "      <td>40.760635</td>\n",
       "      <td>N</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>id0809232</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-01 12:12:25</td>\n",
       "      <td>2016-04-01 12:23:17</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979538</td>\n",
       "      <td>40.753361</td>\n",
       "      <td>-73.963997</td>\n",
       "      <td>40.763458</td>\n",
       "      <td>N</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>id2352683</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-09 03:34:27</td>\n",
       "      <td>2016-04-09 03:41:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.995865</td>\n",
       "      <td>40.758812</td>\n",
       "      <td>-73.993324</td>\n",
       "      <td>40.740322</td>\n",
       "      <td>N</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>id1603037</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-25 10:36:26</td>\n",
       "      <td>2016-06-25 10:55:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.993553</td>\n",
       "      <td>40.747173</td>\n",
       "      <td>-74.006142</td>\n",
       "      <td>40.704384</td>\n",
       "      <td>N</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>id3321406</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-03 08:15:05</td>\n",
       "      <td>2016-06-03 08:56:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.955231</td>\n",
       "      <td>40.777134</td>\n",
       "      <td>-73.788750</td>\n",
       "      <td>40.641472</td>\n",
       "      <td>N</td>\n",
       "      <td>2485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>id0129640</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-14 13:27:56</td>\n",
       "      <td>2016-02-14 13:49:19</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.956581</td>\n",
       "      <td>40.771358</td>\n",
       "      <td>-73.974968</td>\n",
       "      <td>40.732792</td>\n",
       "      <td>N</td>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>id3587298</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-02-27 21:56:01</td>\n",
       "      <td>2016-02-27 22:14:51</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.983765</td>\n",
       "      <td>40.749874</td>\n",
       "      <td>-73.958832</td>\n",
       "      <td>40.800961</td>\n",
       "      <td>N</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>id2104175</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-20 23:07:16</td>\n",
       "      <td>2016-06-20 23:18:50</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.958435</td>\n",
       "      <td>40.713192</td>\n",
       "      <td>-73.949539</td>\n",
       "      <td>40.680252</td>\n",
       "      <td>N</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>id3973319</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-13 21:57:27</td>\n",
       "      <td>2016-06-13 22:12:19</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994217</td>\n",
       "      <td>40.713306</td>\n",
       "      <td>-73.982849</td>\n",
       "      <td>40.692299</td>\n",
       "      <td>N</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>id1410897</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-23 14:10:39</td>\n",
       "      <td>2016-03-23 14:49:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982117</td>\n",
       "      <td>40.756351</td>\n",
       "      <td>-73.865692</td>\n",
       "      <td>40.770988</td>\n",
       "      <td>N</td>\n",
       "      <td>2331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458614</th>\n",
       "      <td>id2061444</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 17:16:07</td>\n",
       "      <td>2016-02-08 17:21:45</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980927</td>\n",
       "      <td>40.767651</td>\n",
       "      <td>-73.965302</td>\n",
       "      <td>40.765251</td>\n",
       "      <td>N</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458615</th>\n",
       "      <td>id3182230</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-02-05 17:57:08</td>\n",
       "      <td>2016-02-05 18:11:25</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991013</td>\n",
       "      <td>40.728321</td>\n",
       "      <td>-73.966766</td>\n",
       "      <td>40.711548</td>\n",
       "      <td>N</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458616</th>\n",
       "      <td>id2822294</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22 17:21:14</td>\n",
       "      <td>2016-04-22 17:29:22</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.988327</td>\n",
       "      <td>40.732147</td>\n",
       "      <td>-73.999641</td>\n",
       "      <td>40.734192</td>\n",
       "      <td>N</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458617</th>\n",
       "      <td>id0820021</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-15 08:31:20</td>\n",
       "      <td>2016-04-15 08:34:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.975433</td>\n",
       "      <td>40.752411</td>\n",
       "      <td>-73.973122</td>\n",
       "      <td>40.746780</td>\n",
       "      <td>N</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458618</th>\n",
       "      <td>id1046767</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-17 01:46:48</td>\n",
       "      <td>2016-04-17 01:52:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.987564</td>\n",
       "      <td>40.733387</td>\n",
       "      <td>-74.001129</td>\n",
       "      <td>40.731056</td>\n",
       "      <td>N</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458619</th>\n",
       "      <td>id1083860</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-23 12:14:15</td>\n",
       "      <td>2016-04-23 12:26:03</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.954773</td>\n",
       "      <td>40.777882</td>\n",
       "      <td>-73.980904</td>\n",
       "      <td>40.782516</td>\n",
       "      <td>N</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458620</th>\n",
       "      <td>id0694577</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-28 20:51:03</td>\n",
       "      <td>2016-04-28 21:10:25</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.966324</td>\n",
       "      <td>40.758072</td>\n",
       "      <td>-74.006516</td>\n",
       "      <td>40.736641</td>\n",
       "      <td>N</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458621</th>\n",
       "      <td>id3267199</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-09 14:33:30</td>\n",
       "      <td>2016-05-09 15:12:45</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.959534</td>\n",
       "      <td>40.782749</td>\n",
       "      <td>-73.990959</td>\n",
       "      <td>40.751091</td>\n",
       "      <td>N</td>\n",
       "      <td>2355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458622</th>\n",
       "      <td>id0125435</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-19 18:26:52</td>\n",
       "      <td>2016-02-19 18:36:04</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.008408</td>\n",
       "      <td>40.721142</td>\n",
       "      <td>-74.000557</td>\n",
       "      <td>40.723911</td>\n",
       "      <td>N</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458623</th>\n",
       "      <td>id3369208</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-18 20:35:30</td>\n",
       "      <td>2016-01-18 20:44:44</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991081</td>\n",
       "      <td>40.737408</td>\n",
       "      <td>-73.987671</td>\n",
       "      <td>40.722622</td>\n",
       "      <td>N</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458624</th>\n",
       "      <td>id3482902</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-01 07:21:04</td>\n",
       "      <td>2016-03-01 07:23:36</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.974693</td>\n",
       "      <td>40.756088</td>\n",
       "      <td>-73.969971</td>\n",
       "      <td>40.762115</td>\n",
       "      <td>N</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458625</th>\n",
       "      <td>id3730733</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-25 17:21:15</td>\n",
       "      <td>2016-01-25 17:54:37</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.989655</td>\n",
       "      <td>40.740612</td>\n",
       "      <td>-73.961029</td>\n",
       "      <td>40.765366</td>\n",
       "      <td>N</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458626</th>\n",
       "      <td>id0155863</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-17 17:21:11</td>\n",
       "      <td>2016-01-17 17:25:15</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.954071</td>\n",
       "      <td>40.767021</td>\n",
       "      <td>-73.950340</td>\n",
       "      <td>40.778233</td>\n",
       "      <td>N</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458627</th>\n",
       "      <td>id0439281</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-23 10:10:28</td>\n",
       "      <td>2016-06-23 10:25:08</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.981651</td>\n",
       "      <td>40.767708</td>\n",
       "      <td>-73.959183</td>\n",
       "      <td>40.777412</td>\n",
       "      <td>N</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458628</th>\n",
       "      <td>id0986544</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-30 03:08:19</td>\n",
       "      <td>2016-05-30 03:14:10</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.988632</td>\n",
       "      <td>40.721378</td>\n",
       "      <td>-73.975548</td>\n",
       "      <td>40.728519</td>\n",
       "      <td>N</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458629</th>\n",
       "      <td>id3109086</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-24 10:33:51</td>\n",
       "      <td>2016-06-24 10:43:52</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.959618</td>\n",
       "      <td>40.808941</td>\n",
       "      <td>-73.947922</td>\n",
       "      <td>40.830189</td>\n",
       "      <td>N</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458630</th>\n",
       "      <td>id0287353</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-25 03:44:32</td>\n",
       "      <td>2016-06-25 03:53:41</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.991508</td>\n",
       "      <td>40.727135</td>\n",
       "      <td>-73.988136</td>\n",
       "      <td>40.740932</td>\n",
       "      <td>N</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458631</th>\n",
       "      <td>id1724231</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-14 23:18:23</td>\n",
       "      <td>2016-05-14 23:24:05</td>\n",
       "      <td>3</td>\n",
       "      <td>-73.958946</td>\n",
       "      <td>40.763725</td>\n",
       "      <td>-73.953156</td>\n",
       "      <td>40.780003</td>\n",
       "      <td>N</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458632</th>\n",
       "      <td>id0469946</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-06 11:04:48</td>\n",
       "      <td>2016-03-06 11:17:45</td>\n",
       "      <td>2</td>\n",
       "      <td>-74.015572</td>\n",
       "      <td>40.710892</td>\n",
       "      <td>-73.996620</td>\n",
       "      <td>40.743633</td>\n",
       "      <td>N</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458633</th>\n",
       "      <td>id2432342</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-03-17 19:10:16</td>\n",
       "      <td>2016-03-17 19:26:35</td>\n",
       "      <td>3</td>\n",
       "      <td>-73.979652</td>\n",
       "      <td>40.735279</td>\n",
       "      <td>-73.995522</td>\n",
       "      <td>40.759754</td>\n",
       "      <td>N</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458634</th>\n",
       "      <td>id3445276</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-03 13:51:25</td>\n",
       "      <td>2016-04-03 14:07:37</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.989075</td>\n",
       "      <td>40.730465</td>\n",
       "      <td>-73.963882</td>\n",
       "      <td>40.773739</td>\n",
       "      <td>N</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458635</th>\n",
       "      <td>id3027038</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-19 14:46:55</td>\n",
       "      <td>2016-05-19 14:50:52</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.985390</td>\n",
       "      <td>40.763020</td>\n",
       "      <td>-73.989708</td>\n",
       "      <td>40.767502</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458636</th>\n",
       "      <td>id0405770</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-12 10:13:06</td>\n",
       "      <td>2016-02-12 10:26:26</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.863815</td>\n",
       "      <td>40.769684</td>\n",
       "      <td>-73.864395</td>\n",
       "      <td>40.761326</td>\n",
       "      <td>N</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458637</th>\n",
       "      <td>id1920898</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-17 18:48:16</td>\n",
       "      <td>2016-04-17 19:00:56</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.975357</td>\n",
       "      <td>40.751705</td>\n",
       "      <td>-73.949478</td>\n",
       "      <td>40.776764</td>\n",
       "      <td>N</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458638</th>\n",
       "      <td>id1454193</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-02 00:39:39</td>\n",
       "      <td>2016-02-02 00:46:33</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.988823</td>\n",
       "      <td>40.736553</td>\n",
       "      <td>-73.989166</td>\n",
       "      <td>40.757393</td>\n",
       "      <td>N</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458639</th>\n",
       "      <td>id2376096</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-08 13:31:04</td>\n",
       "      <td>2016-04-08 13:44:02</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.982201</td>\n",
       "      <td>40.745522</td>\n",
       "      <td>-73.994911</td>\n",
       "      <td>40.740170</td>\n",
       "      <td>N</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458640</th>\n",
       "      <td>id1049543</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-10 07:35:15</td>\n",
       "      <td>2016-01-10 07:46:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.000946</td>\n",
       "      <td>40.747379</td>\n",
       "      <td>-73.970184</td>\n",
       "      <td>40.796547</td>\n",
       "      <td>N</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458641</th>\n",
       "      <td>id2304944</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-22 06:57:41</td>\n",
       "      <td>2016-04-22 07:10:25</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.959129</td>\n",
       "      <td>40.768799</td>\n",
       "      <td>-74.004433</td>\n",
       "      <td>40.707371</td>\n",
       "      <td>N</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458642</th>\n",
       "      <td>id2714485</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-05 15:56:26</td>\n",
       "      <td>2016-01-05 16:02:39</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982079</td>\n",
       "      <td>40.749062</td>\n",
       "      <td>-73.974632</td>\n",
       "      <td>40.757107</td>\n",
       "      <td>N</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458643</th>\n",
       "      <td>id1209952</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-05 14:44:25</td>\n",
       "      <td>2016-04-05 14:47:43</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979538</td>\n",
       "      <td>40.781750</td>\n",
       "      <td>-73.972809</td>\n",
       "      <td>40.790585</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1458644 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0        id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1        id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2        id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3        id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4        id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "5        id0801584          2  2016-01-30 22:01:40  2016-01-30 22:09:03   \n",
       "6        id1813257          1  2016-06-17 22:34:59  2016-06-17 22:40:40   \n",
       "7        id1324603          2  2016-05-21 07:54:58  2016-05-21 08:20:49   \n",
       "8        id1301050          1  2016-05-27 23:12:23  2016-05-27 23:16:38   \n",
       "9        id0012891          2  2016-03-10 21:45:01  2016-03-10 22:05:26   \n",
       "10       id1436371          2  2016-05-10 22:08:41  2016-05-10 22:29:55   \n",
       "11       id1299289          2  2016-05-15 11:16:11  2016-05-15 11:34:59   \n",
       "12       id1187965          2  2016-02-19 09:52:46  2016-02-19 10:11:20   \n",
       "13       id0799785          2  2016-06-01 20:58:29  2016-06-01 21:02:49   \n",
       "14       id2900608          2  2016-05-27 00:43:36  2016-05-27 01:07:10   \n",
       "15       id3319787          1  2016-05-16 15:29:02  2016-05-16 15:32:33   \n",
       "16       id3379579          2  2016-04-11 17:29:50  2016-04-11 18:08:26   \n",
       "17       id1154431          1  2016-04-14 08:48:26  2016-04-14 09:00:37   \n",
       "18       id3552682          1  2016-06-27 09:55:13  2016-06-27 10:17:10   \n",
       "19       id3390316          2  2016-06-05 13:47:23  2016-06-05 13:51:34   \n",
       "20       id2070428          1  2016-02-28 02:23:02  2016-02-28 02:31:08   \n",
       "21       id0809232          2  2016-04-01 12:12:25  2016-04-01 12:23:17   \n",
       "22       id2352683          1  2016-04-09 03:34:27  2016-04-09 03:41:30   \n",
       "23       id1603037          1  2016-06-25 10:36:26  2016-06-25 10:55:49   \n",
       "24       id3321406          2  2016-06-03 08:15:05  2016-06-03 08:56:30   \n",
       "25       id0129640          2  2016-02-14 13:27:56  2016-02-14 13:49:19   \n",
       "26       id3587298          1  2016-02-27 21:56:01  2016-02-27 22:14:51   \n",
       "27       id2104175          1  2016-06-20 23:07:16  2016-06-20 23:18:50   \n",
       "28       id3973319          2  2016-06-13 21:57:27  2016-06-13 22:12:19   \n",
       "29       id1410897          1  2016-03-23 14:10:39  2016-03-23 14:49:30   \n",
       "...            ...        ...                  ...                  ...   \n",
       "1458614  id2061444          2  2016-02-08 17:16:07  2016-02-08 17:21:45   \n",
       "1458615  id3182230          1  2016-02-05 17:57:08  2016-02-05 18:11:25   \n",
       "1458616  id2822294          1  2016-04-22 17:21:14  2016-04-22 17:29:22   \n",
       "1458617  id0820021          2  2016-04-15 08:31:20  2016-04-15 08:34:48   \n",
       "1458618  id1046767          2  2016-04-17 01:46:48  2016-04-17 01:52:55   \n",
       "1458619  id1083860          2  2016-04-23 12:14:15  2016-04-23 12:26:03   \n",
       "1458620  id0694577          2  2016-04-28 20:51:03  2016-04-28 21:10:25   \n",
       "1458621  id3267199          2  2016-05-09 14:33:30  2016-05-09 15:12:45   \n",
       "1458622  id0125435          2  2016-02-19 18:26:52  2016-02-19 18:36:04   \n",
       "1458623  id3369208          1  2016-01-18 20:35:30  2016-01-18 20:44:44   \n",
       "1458624  id3482902          1  2016-03-01 07:21:04  2016-03-01 07:23:36   \n",
       "1458625  id3730733          2  2016-01-25 17:21:15  2016-01-25 17:54:37   \n",
       "1458626  id0155863          2  2016-01-17 17:21:11  2016-01-17 17:25:15   \n",
       "1458627  id0439281          2  2016-06-23 10:10:28  2016-06-23 10:25:08   \n",
       "1458628  id0986544          2  2016-05-30 03:08:19  2016-05-30 03:14:10   \n",
       "1458629  id3109086          2  2016-06-24 10:33:51  2016-06-24 10:43:52   \n",
       "1458630  id0287353          2  2016-06-25 03:44:32  2016-06-25 03:53:41   \n",
       "1458631  id1724231          1  2016-05-14 23:18:23  2016-05-14 23:24:05   \n",
       "1458632  id0469946          2  2016-03-06 11:04:48  2016-03-06 11:17:45   \n",
       "1458633  id2432342          1  2016-03-17 19:10:16  2016-03-17 19:26:35   \n",
       "1458634  id3445276          1  2016-04-03 13:51:25  2016-04-03 14:07:37   \n",
       "1458635  id3027038          2  2016-05-19 14:46:55  2016-05-19 14:50:52   \n",
       "1458636  id0405770          2  2016-02-12 10:13:06  2016-02-12 10:26:26   \n",
       "1458637  id1920898          1  2016-04-17 18:48:16  2016-04-17 19:00:56   \n",
       "1458638  id1454193          2  2016-02-02 00:39:39  2016-02-02 00:46:33   \n",
       "1458639  id2376096          2  2016-04-08 13:31:04  2016-04-08 13:44:02   \n",
       "1458640  id1049543          1  2016-01-10 07:35:15  2016-01-10 07:46:10   \n",
       "1458641  id2304944          2  2016-04-22 06:57:41  2016-04-22 07:10:25   \n",
       "1458642  id2714485          1  2016-01-05 15:56:26  2016-01-05 16:02:39   \n",
       "1458643  id1209952          1  2016-04-05 14:44:25  2016-04-05 14:47:43   \n",
       "\n",
       "         passenger_count  pickup_longitude  pickup_latitude  \\\n",
       "0                      1        -73.982155        40.767937   \n",
       "1                      1        -73.980415        40.738564   \n",
       "2                      1        -73.979027        40.763939   \n",
       "3                      1        -74.010040        40.719971   \n",
       "4                      1        -73.973053        40.793209   \n",
       "5                      6        -73.982857        40.742195   \n",
       "6                      4        -73.969017        40.757839   \n",
       "7                      1        -73.969276        40.797779   \n",
       "8                      1        -73.999481        40.738400   \n",
       "9                      1        -73.981049        40.744339   \n",
       "10                     1        -73.982651        40.763840   \n",
       "11                     4        -73.991531        40.749439   \n",
       "12                     2        -73.962982        40.756680   \n",
       "13                     1        -73.956306        40.767941   \n",
       "14                     1        -73.992195        40.727226   \n",
       "15                     1        -73.955513        40.768593   \n",
       "16                     1        -73.991165        40.755562   \n",
       "17                     1        -73.994255        40.745804   \n",
       "18                     1        -74.003983        40.713013   \n",
       "19                     1        -73.983887        40.738197   \n",
       "20                     1        -73.980370        40.742420   \n",
       "21                     1        -73.979538        40.753361   \n",
       "22                     1        -73.995865        40.758812   \n",
       "23                     1        -73.993553        40.747173   \n",
       "24                     1        -73.955231        40.777134   \n",
       "25                     1        -73.956581        40.771358   \n",
       "26                     1        -73.983765        40.749874   \n",
       "27                     1        -73.958435        40.713192   \n",
       "28                     1        -73.994217        40.713306   \n",
       "29                     1        -73.982117        40.756351   \n",
       "...                  ...               ...              ...   \n",
       "1458614                1        -73.980927        40.767651   \n",
       "1458615                1        -73.991013        40.728321   \n",
       "1458616                1        -73.988327        40.732147   \n",
       "1458617                1        -73.975433        40.752411   \n",
       "1458618                1        -73.987564        40.733387   \n",
       "1458619                1        -73.954773        40.777882   \n",
       "1458620                1        -73.966324        40.758072   \n",
       "1458621                1        -73.959534        40.782749   \n",
       "1458622                1        -74.008408        40.721142   \n",
       "1458623                1        -73.991081        40.737408   \n",
       "1458624                1        -73.974693        40.756088   \n",
       "1458625                1        -73.989655        40.740612   \n",
       "1458626                2        -73.954071        40.767021   \n",
       "1458627                5        -73.981651        40.767708   \n",
       "1458628                2        -73.988632        40.721378   \n",
       "1458629                1        -73.959618        40.808941   \n",
       "1458630                5        -73.991508        40.727135   \n",
       "1458631                3        -73.958946        40.763725   \n",
       "1458632                2        -74.015572        40.710892   \n",
       "1458633                3        -73.979652        40.735279   \n",
       "1458634                2        -73.989075        40.730465   \n",
       "1458635                1        -73.985390        40.763020   \n",
       "1458636                1        -73.863815        40.769684   \n",
       "1458637                1        -73.975357        40.751705   \n",
       "1458638                5        -73.988823        40.736553   \n",
       "1458639                4        -73.982201        40.745522   \n",
       "1458640                1        -74.000946        40.747379   \n",
       "1458641                1        -73.959129        40.768799   \n",
       "1458642                1        -73.982079        40.749062   \n",
       "1458643                1        -73.979538        40.781750   \n",
       "\n",
       "         dropoff_longitude  dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0               -73.964630         40.765602                  N            455  \n",
       "1               -73.999481         40.731152                  N            663  \n",
       "2               -74.005333         40.710087                  N           2124  \n",
       "3               -74.012268         40.706718                  N            429  \n",
       "4               -73.972923         40.782520                  N            435  \n",
       "5               -73.992081         40.749184                  N            443  \n",
       "6               -73.957405         40.765896                  N            341  \n",
       "7               -73.922470         40.760559                  N           1551  \n",
       "8               -73.985786         40.732815                  N            255  \n",
       "9               -73.973000         40.789989                  N           1225  \n",
       "10              -74.002228         40.732990                  N           1274  \n",
       "11              -73.956543         40.770630                  N           1128  \n",
       "12              -73.984406         40.760719                  N           1114  \n",
       "13              -73.966110         40.763000                  N            260  \n",
       "14              -73.974655         40.783070                  N           1414  \n",
       "15              -73.948761         40.771545                  N            211  \n",
       "16              -73.999290         40.725353                  N           2316  \n",
       "17              -73.999657         40.723343                  N            731  \n",
       "18              -73.979195         40.749924                  N           1317  \n",
       "19              -73.991203         40.727871                  N            251  \n",
       "20              -73.962852         40.760635                  N            486  \n",
       "21              -73.963997         40.763458                  N            652  \n",
       "22              -73.993324         40.740322                  N            423  \n",
       "23              -74.006142         40.704384                  N           1163  \n",
       "24              -73.788750         40.641472                  N           2485  \n",
       "25              -73.974968         40.732792                  N           1283  \n",
       "26              -73.958832         40.800961                  N           1130  \n",
       "27              -73.949539         40.680252                  N            694  \n",
       "28              -73.982849         40.692299                  N            892  \n",
       "29              -73.865692         40.770988                  N           2331  \n",
       "...                    ...               ...                ...            ...  \n",
       "1458614         -73.965302         40.765251                  N            338  \n",
       "1458615         -73.966766         40.711548                  N            857  \n",
       "1458616         -73.999641         40.734192                  N            488  \n",
       "1458617         -73.973122         40.746780                  N            208  \n",
       "1458618         -74.001129         40.731056                  N            367  \n",
       "1458619         -73.980904         40.782516                  N            708  \n",
       "1458620         -74.006516         40.736641                  N           1162  \n",
       "1458621         -73.990959         40.751091                  N           2355  \n",
       "1458622         -74.000557         40.723911                  N            552  \n",
       "1458623         -73.987671         40.722622                  N            554  \n",
       "1458624         -73.969971         40.762115                  N            152  \n",
       "1458625         -73.961029         40.765366                  N           2002  \n",
       "1458626         -73.950340         40.778233                  N            244  \n",
       "1458627         -73.959183         40.777412                  N            880  \n",
       "1458628         -73.975548         40.728519                  N            351  \n",
       "1458629         -73.947922         40.830189                  N            601  \n",
       "1458630         -73.988136         40.740932                  N            549  \n",
       "1458631         -73.953156         40.780003                  N            342  \n",
       "1458632         -73.996620         40.743633                  N            777  \n",
       "1458633         -73.995522         40.759754                  N            979  \n",
       "1458634         -73.963882         40.773739                  N            972  \n",
       "1458635         -73.989708         40.767502                  N            237  \n",
       "1458636         -73.864395         40.761326                  N            800  \n",
       "1458637         -73.949478         40.776764                  N            760  \n",
       "1458638         -73.989166         40.757393                  N            414  \n",
       "1458639         -73.994911         40.740170                  N            778  \n",
       "1458640         -73.970184         40.796547                  N            655  \n",
       "1458641         -74.004433         40.707371                  N            764  \n",
       "1458642         -73.974632         40.757107                  N            373  \n",
       "1458643         -73.972809         40.790585                  N            198  \n",
       "\n",
       "[1458644 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peek executes the subgraph necessary to produce the desired artifact, and preview it, \n",
    "#recording the event that the user viewed the Artifact.\n",
    "tr_data_df.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def calculate_distance(data_df):\n",
    "    def manhattan_distance(x1, y1, x2, y2):\n",
    "        return abs(x1 - x2) + abs(y1 - y2)\n",
    "    data_df['distance'] = [ i for i in map(manhattan_distance, data_df['pickup_longitude'], data_df['pickup_latitude'], \n",
    "                                           data_df['dropoff_longitude'], data_df['dropoff_latitude'])]\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor action that maps the input parameters of 'tr_data_df' to the function 'calculate_distance'\n",
    "do_calc_dist = ex.action(calculate_distance, [tr_data_df])\n",
    "\n",
    "#Creates a Flor artifact that corresponds to the output of 'do_calc_dist' \n",
    "#and is stored in the file 'train_dist_df.pkl'\n",
    "tr_data_dist_df = ex.artifact('train_dist_df.pkl', do_calc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def preproc(train_data):\n",
    "    import numpy as np\n",
    "    # https://www.kaggle.com/stephaniestallworth/nyc-taxi-eda-regression-fivethirtyeight-viz/notebook\n",
    "    train_data = train_data[train_data['passenger_count']>0]\n",
    "    train_data = train_data[train_data['passenger_count']<9]\n",
    "\n",
    "    # Remove coordinate outliers\n",
    "    train_data = train_data[train_data['pickup_longitude'] <= -73.75]\n",
    "    train_data = train_data[train_data['pickup_longitude'] >= -74.03]\n",
    "    train_data = train_data[train_data['pickup_latitude'] <= 40.85]\n",
    "    train_data = train_data[train_data['pickup_latitude'] >= 40.63]\n",
    "    train_data = train_data[train_data['dropoff_longitude'] <= -73.75]\n",
    "    train_data = train_data[train_data['dropoff_longitude'] >= -74.03]\n",
    "    train_data = train_data[train_data['dropoff_latitude'] <= 40.85]\n",
    "    train_data = train_data[train_data['dropoff_latitude'] >= 40.63]\n",
    "\n",
    "    # Remove trip_duration outliers\n",
    "    trip_duration_mean = np.mean(train_data['trip_duration'])\n",
    "    trip_duration_std = np.std(train_data['trip_duration'])\n",
    "    train_data = train_data[train_data['trip_duration']<=trip_duration_mean + 2*trip_duration_std]\n",
    "    train_data = train_data[train_data['trip_duration']>= trip_duration_mean - 2*trip_duration_std]\n",
    "    train_data = train_data[train_data['trip_duration'] >= 30]\n",
    "    train_data = train_data[train_data['trip_duration'] <= 60*240]\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor action that maps the input parameters of 'tr_data_dist_df' to the function 'preproc'\n",
    "do_preproc = ex.action(preproc, [tr_data_dist_df])\n",
    "\n",
    "#Creates a Flor artifact that corresponds to the output of 'do_preproc' \n",
    "#and is stored in the file 'train_ready.pkl'\n",
    "tr_ready = ex.artifact('train_ready.pkl', do_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def split(data_df):\n",
    "    X = data_df[['vendor_id', 'passenger_count', 'pickup_longitude',\n",
    "        'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "       'store_and_fwd_flag', 'pickup_datetime', 'distance']]\n",
    "    y = data_df['trip_duration']\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor action that maps the input parameters of 'tr_ready' to the function 'split'\n",
    "do_split = ex.action(split, [tr_ready])\n",
    "\n",
    "#Create multiple Flor artifacts that correspond to the outputs of the function specified in 'do_split'\n",
    "xTrain = ex.artifact('xTrain.pkl', do_split)\n",
    "xTest = ex.artifact('xTest.pkl', do_split)\n",
    "yTrain = ex.artifact('yTrain.pkl', do_split)\n",
    "yTest = ex.artifact('yTest.pkl', do_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def train(data_df, trainingy, num_estimators):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    data_df['duration'] = trainingy\n",
    "\n",
    "    def roundtime(tstring):\n",
    "        hours, mins, secs = tstring.split(':')\n",
    "        if int(mins) >= 30:\n",
    "            if hours == '23':\n",
    "                return '00'\n",
    "            else:\n",
    "                return str(int(hours) + 1)\n",
    "        else:\n",
    "            return hours\n",
    "\n",
    "    def weekday(start):\n",
    "        from datetime import datetime\n",
    "        fmt = '%Y-%m-%d %H:%M:%S'\n",
    "        tstamp = datetime.strptime(start, fmt)\n",
    "        return int(tstamp.weekday())\n",
    "\n",
    "    data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "    data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "    data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "    import math\n",
    "\n",
    "    clf = RandomForestRegressor(n_estimators=20, n_jobs=3)\n",
    "    clf.fit(data_df[['vendor_id', 'start_hr', 'start_month', 'start_weekday', 'distance', \n",
    "                     'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "                    'dropoff_latitude']].values, data_df['duration'].values )\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Flor literal that corresponds to the array specified by the first parameter\n",
    "num_est = ex.literal([15, 20, 30], 'num_estimators')\n",
    "\n",
    "#Changes the configuration of the Flor literal to be evaluated element-wise of the array specified above\n",
    "num_est.forEach()\n",
    "\n",
    "#Create a Flor action that maps the input parameters of 'xTrain', 'yTrain' \n",
    "# and 'num_est' to the function 'train'\n",
    "do_train = ex.action(train, [xTrain, yTrain, num_est])\n",
    "\n",
    "#Creates a Flor artifact that corresponds to the output of 'do_train' \n",
    "#and is stored in the file 'model.pkl'\n",
    "model = ex.artifact('model.pkl', do_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap existing functions with the @flor.func decorator so they are able to be referenced by Flor actions\n",
    "@flor.func\n",
    "def test(model, data_df, testingy):\n",
    "    import numpy as np\n",
    "    data_df['duration'] = testingy\n",
    "    def roundtime(tstring):\n",
    "        hours, mins, secs = tstring.split(':')\n",
    "        if int(mins) >= 30:\n",
    "            if hours == '23':\n",
    "                return '00'\n",
    "            else:\n",
    "                return str(int(hours) + 1)\n",
    "        else:\n",
    "            return hours\n",
    "    def weekday(start):\n",
    "        from datetime import datetime\n",
    "        fmt = '%Y-%m-%d %H:%M:%S'\n",
    "        tstamp = datetime.strptime(start, fmt)\n",
    "        return int(tstamp.weekday())\n",
    "\n",
    "    data_df['start_hr'] = data_df['pickup_datetime'].apply(lambda x: int(roundtime(x.split(' ')[1])))\n",
    "    data_df['start_month'] = data_df['pickup_datetime'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n",
    "    data_df['start_weekday']= data_df['pickup_datetime'].apply(lambda x: weekday(x))\n",
    "\n",
    "\n",
    "    preds = model.predict(data_df[['vendor_id', 'start_hr', 'start_month', 'start_weekday', 'distance', \n",
    "                     'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "                    'dropoff_latitude']].values)\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    score = metrics.explained_variance_score(data_df['duration'].values, preds)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(data_df['duration'].values, preds))\n",
    "    return str(score), str(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eric/flor.d\n"
     ]
    }
   ],
   "source": [
    "print(ex.xp_state.versioningDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1f42019c8873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Exit the experiment which commits any changes to Git and Ground\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/flor/experiment.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/flor/above_ground.py\u001b[0m in \u001b[0;36mcommit\u001b[0;34m(xp_state)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 'checksum': {\n\u001b[1;32m    130\u001b[0m                     \u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'checksum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'STRING'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 }\n",
      "\u001b[0;32m~/Research/flor/util.py\u001b[0m in \u001b[0;36mmd5\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Credit: https://stackoverflow.com/a/3431838\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mhash_md5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mhash_md5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "#Create a Flor action that maps the input parameters of 'xTest', 'yTest' \n",
    "# and 'model' to the function 'test'\n",
    "do_test = ex.action(test, [model, xTest, yTest])\n",
    "\n",
    "#Create multiple Flor artifacts that correspond to the outputs of the function specified in 'do_test'\n",
    "score = ex.artifact('score.txt', do_test)\n",
    "rmse = ex.artifact('rmse.txt', do_test)\n",
    "\n",
    "#Exit the experiment which commits any changes to Git and Ground\n",
    "ex.__exit__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(ex.xp_state.versioningDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plate_demo']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot() creates a graph representation of the lineage to a particular artifact\n",
    "score.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
