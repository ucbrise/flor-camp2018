{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp 2018: Introduction to Flor!\n",
    "\n",
    "\n",
    "Welcome to RISE Camp 2018! Flor is a system for managing workflow development within the machine learning lifecycle. This tool enables data scientists to describe ML workflows as directed acyclic graphs (DAGs) of Actions, Artifacts, or Literals and to experiment with different configurations quickly by running multi-trial experiments. \n",
    "\n",
    "The purpose of this notebook is to help you use Flor in order to naviagte through different parts of the data science lifecycle.\n",
    "\n",
    "As you work through this notebook, you will learn:\n",
    "\n",
    "* How to define/use experiments, literals, artifacts and actions.\n",
    "* How to run experiments with different congigurations.\n",
    "* Compare models with other past versions in order to select the best model.\n",
    "\n",
    "We will be working with a ratings dataset. \n",
    "\n",
    "**Data science is a collaborative activity - we encourage you to work with those around you and ask questions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/rise/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scipy.sparse\n",
    "import flor\n",
    "\n",
    "#Pre-processing imports\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Model training and testing imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import average_precision_score, recall_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Before building our model, we will define a pipeline to pre-process our text data. We have used the following techniques to pre-process the text reviews:\n",
    "* Removal of Stop Words\n",
    "* Stemming (reducing inflected words to their stem)\n",
    "* Lemmatization (group together inflected forms of a words)\n",
    "\n",
    "We will wrap the following functions with the @flor.func decorator so they are able to be referenced by Flor actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def preprocessing(data_loc, intermediate_X, intermediate_y, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Data Preprocessing\n",
    "\n",
    "    '''\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "#     data = pd.read_json(data_loc)\n",
    "#     X = data['text']\n",
    "#     y = data['rating'].astype(np.float64)\n",
    "    \n",
    "#     en_stop = get_stop_words('en')\n",
    "\n",
    "#     def filter_sentence(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [word for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in en_stop]\n",
    "#         tokens = stem_words(tokens)\n",
    "#         tokens = lemma_words(tokens)\n",
    "\n",
    "#         ret_str = \" \".join(tokens) \n",
    "\n",
    "#         return ret_str \n",
    "\n",
    "\n",
    "#     #Credit to https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "#     #for stem_words and lemma_words\n",
    "#     def stem_words(words):\n",
    "#         stemmer = PorterStemmer()\n",
    "#         stems = []\n",
    "#         for word in words:\n",
    "#             stem = stemmer.stem(word)\n",
    "#             stems.append(stem)\n",
    "#         return stems\n",
    "\n",
    "#     def lemma_words(words):\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         lemmas = []\n",
    "#         for word in words:\n",
    "#             lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#             lemmas.append(lemma)\n",
    "#         return lemmas\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     X = [filter_sentence(el) for el in X]\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#     y_new = []\n",
    "#     for el in y:\n",
    "#         ret = 0\n",
    "#         if el <= 5:\n",
    "#             ret = 0\n",
    "#         else:\n",
    "#             ret = 1\n",
    "#         y_new.append(ret)\n",
    "#     y = y_new\n",
    "\n",
    "    # Load the cleaned data\n",
    "    with open('data_clean_X.json') as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open('data_clean_y.json') as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "\n",
    "    with open(intermediate_X, 'w') as outfile:\n",
    "       json.dump(X, outfile)\n",
    "    with open(intermediate_y, 'w') as outfile:\n",
    "       json.dump(y, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "We create a flor function to split our data into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def traintest_split(intermediate_X, intermediate_y, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Flor function to perform train/test split.\n",
    "\n",
    "    '''\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "#     def train_drop(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [el for el in tokens if random.random() >= 0.75]\n",
    "#         ret_str = \" \".join(tokens) \n",
    "#         return ret_str \n",
    "#     X_tr = [train_drop(el) for el in X_tr]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    start_time = time.time()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    with open(y_train, 'w') as outfile:\n",
    "        json.dump(y_tr, outfile)\n",
    "    with open(y_test, 'w') as outfile:\n",
    "        json.dump(y_te, outfile)\n",
    "\n",
    "    print(\"saving sparse matrices\")\n",
    "    scipy.sparse.save_npz(X_train, X_tr)\n",
    "    scipy.sparse.save_npz(X_test, X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Now, we can create a Flor function to train and evaluate a model to classify reviews into rating buckets. Notice that we pass in `hyperparameters` in addition to the train and test data. These hyperparameters will allow us to tune our model and track results with ease; we will define them later in our experiment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_test(X_train, X_test, y_train, y_test, hyperparameters, precision, recall, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Flor function to train and evaluate model.\n",
    "\n",
    "    '''\n",
    "    print(\"Loading Data\")\n",
    "    X_train = scipy.sparse.load_npz(X_train)\n",
    "    X_test = scipy.sparse.load_npz(X_test)\n",
    "    with open(y_train) as json_data:\n",
    "        y_train = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(y_test) as json_data:\n",
    "        y_test = json.load(json_data)\n",
    "        json_data.close()\n",
    "    print(\"Training Model\")\n",
    "    \n",
    "    #Either train Random Forest or Multi-layer Perception Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=hyperparameters).fit(X_train, y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1).fit(X_train, y_train)\n",
    "    #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1).fit(X_train, y_train)    \n",
    "    #clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1, max_iter = 1000).fit(X_train, y_train)\n",
    "    #clf = MultinomialNB().fit(X_train, y_train)\n",
    "    clf = DecisionTreeClassifier(splitter=hyperparameters).fit(X_train, y_train)\n",
    "    #clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "    print(\"Predicting Model\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Writing Results\") \n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    #Write the precision to the output file\n",
    "    output = open(precision, 'w')\n",
    "    output.write(str(hyperparameters) + '\\n')\n",
    "    output.write(str(prec))\n",
    "    output.close()\n",
    "    \n",
    "    #Write the recall to the output file\n",
    "    output = open(recall, 'w')\n",
    "    output.write(str(hyperparameters) + '\\n')\n",
    "    output.write(str(rec))\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "Finally, we will now define our Flor experiment using the Flor functions we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment(\"risecamp_demo\") as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "    \n",
    "    # Defines preprocessing action and resulting intermediary artifacts\n",
    "    data = ex.artifact('data.json', 'data_loc')\n",
    "    preprocessing = ex.action(preprocessing, [data])\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', preprocessing)\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', preprocessing)\n",
    "    \n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "    \n",
    "    #Define the hyperparameters for the models\n",
    "    random_forest_Nestimators = ex.literalForEach(v=[25, 50, 75], name=\"hyperparameters\", default=50)\n",
    "    #MLP_hidden_layer_size = ex.literalForEach(v=[(1, ), (2, ), (3, )], name=\"hyperparameters\", default=(2, ))\n",
    "    #k_neighbors_num = ex.literalForEach(v=[2, 5, 8], name=\"hyperparameters\", default=5)\n",
    "    \n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    #change to MLP_hidden_layer_size \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, decision_tree_splitter])\n",
    "    #for naive bayes\n",
    "#     do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    precision = ex.artifact('precision.txt', 'precision', do_test)\n",
    "    recall = ex.artifact('recall.txt', 'recall', do_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: source Pages: 1 -->\n",
       "<svg width=\"525pt\" height=\"495pt\"\n",
       " viewBox=\"0.00 0.00 524.69 495.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 491)\">\n",
       "<title>source</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-491 520.6855,-491 520.6855,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster2hyperparameters</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"122.1855,-8 122.1855,-155 306.1855,-155 306.1855,-8 122.1855,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.1855\" y=\"-139.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2x</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"102.1855,-487 12.1855,-487 12.1855,-451 102.1855,-451 102.1855,-487\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.1855\" y=\"-465.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">tutorial.ipynb</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"167.1855\" cy=\"-253\" rx=\"58.4896\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.1855\" y=\"-249.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">traintest_split</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M55.0579,-450.7583C52.2911,-418.8349 50.8448,-351.9629 80.1855,-307 90.0121,-291.9413 105.6553,-280.3522 120.9846,-271.8419\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.6754,-274.9076 129.9277,-267.1839 119.4418,-268.6992 122.6754,-274.9076\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"151.1855\" cy=\"-397\" rx=\"59.5901\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.1855\" y=\"-393.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">preprocessing</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;11 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.9056,-450.8314C93.041,-441.5362 107.9465,-430.1192 120.9031,-420.195\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"123.2767,-422.7857 129.0872,-413.9263 119.0202,-417.2285 123.2767,-422.7857\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"214.1855\" cy=\"-106\" rx=\"43.5923\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.1855\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">train_test</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M45.2513,-450.7554C29.0753,-424.2192 2.1855,-372.903 2.1855,-325 2.1855,-325 2.1855,-325 2.1855,-253 2.1855,-212.1184 -6.5564,-193.0283 21.1855,-163 40.4823,-142.1127 113.2763,-124.7415 163.7295,-114.8604\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.6526,-118.2472 173.8143,-112.9261 163.334,-111.3725 164.6526,-118.2472\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"185.6855,-487 120.6855,-487 120.6855,-451 185.6855,-451 185.6855,-487\"/>\n",
       "<text text-anchor=\"middle\" x=\"153.1855\" y=\"-465.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data.json</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;11 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>12&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.6808,-450.8314C152.4669,-443.131 152.2125,-433.9743 151.9748,-425.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"155.4734,-425.3122 151.6969,-415.4133 148.4761,-425.5066 155.4734,-425.3122\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"516.6855,-199 409.6855,-199 409.6855,-163 516.6855,-163 516.6855,-199\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"409.6855,-163 516.6855,-163 \"/>\n",
       "<text text-anchor=\"middle\" x=\"463.1855\" y=\"-177.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hyperparameters</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>13&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M409.4627,-164.8185C364.5939,-151.3038 301.3599,-132.2574 259.1337,-119.5386\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"259.988,-116.1407 249.4034,-116.6078 257.9691,-122.8432 259.988,-116.1407\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"301.1855,-199 223.1855,-199 223.1855,-163 301.1855,-163 301.1855,-199\"/>\n",
       "<text text-anchor=\"middle\" x=\"262.1855\" y=\"-177.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x_train.npz</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;4 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>8&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M189.2191,-236.3008C201.3841,-227.081 216.7129,-215.4634 230.1769,-205.2591\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"232.4186,-207.9518 238.2743,-199.1222 228.1905,-202.373 232.4186,-207.9518\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"391.1855,-199 319.1855,-199 319.1855,-163 391.1855,-163 391.1855,-199\"/>\n",
       "<text text-anchor=\"middle\" x=\"355.1855\" y=\"-177.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x_test.npz</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;5 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>8&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M204.5966,-239.0994C231.403,-229.0884 268.5886,-215.0985 309.2884,-199.3193\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.8065,-202.4844 318.8601,-195.6004 308.2713,-195.9596 310.8065,-202.4844\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"111.6855,-199 30.6855,-199 30.6855,-163 111.6855,-163 111.6855,-199\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.1855\" y=\"-177.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y_train.json</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;6 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>8&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M144.9198,-236.3008C132.6268,-227.081 117.1367,-215.4634 103.5309,-205.2591\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.4484,-202.3222 95.3483,-199.1222 101.2483,-207.9222 105.4484,-202.3222\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"204.6855,-199 129.6855,-199 129.6855,-163 204.6855,-163 204.6855,-199\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.1855\" y=\"-177.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y_test.json</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;7 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M167.1855,-234.8314C167.1855,-227.131 167.1855,-217.9743 167.1855,-209.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.6856,-209.4132 167.1855,-199.4133 163.6856,-209.4133 170.6856,-209.4132\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"206.6855,-343 89.6855,-343 89.6855,-307 206.6855,-307 206.6855,-343\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.1855\" y=\"-321.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data_clean_X.json</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M150.4284,-378.8314C150.1076,-371.131 149.7261,-361.9743 149.3695,-353.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"152.866,-353.2589 148.9527,-343.4133 145.8721,-353.5503 152.866,-353.2589\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"339.1855,-343 225.1855,-343 225.1855,-307 339.1855,-307 339.1855,-343\"/>\n",
       "<text text-anchor=\"middle\" x=\"282.1855\" y=\"-321.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">data_clean_y.json</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M179.934,-381.1993C197.6992,-371.4352 220.7975,-358.7399 240.5445,-347.8866\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"242.2728,-350.9306 249.3505,-343.0467 238.9011,-344.7961 242.2728,-350.9306\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M250.566,-162.8446C244.6432,-153.5902 237.3351,-142.1713 230.8264,-132.0015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"233.7059,-130.0078 225.3674,-123.4718 227.81,-133.7812 233.7059,-130.0078\"/>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M321.0533,-162.8446C299.5162,-151.3887 271.7436,-136.616 249.9088,-125.0018\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.4225,-121.8426 240.9501,-120.2365 248.1351,-128.0227 251.4225,-121.8426\"/>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;2 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>6&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.8018,-162.8446C127.6443,-151.3887 155.8109,-136.616 177.9554,-125.0018\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"179.811,-127.9808 187.0412,-120.2365 176.5596,-121.7817 179.811,-127.9808\"/>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;2 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>7&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M178.5628,-162.8446C184.3623,-153.5902 191.5181,-142.1713 197.8912,-132.0015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.8921,-133.804 203.2365,-123.4718 194.9605,-130.0869 200.8921,-133.804\"/>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.98,-306.8314C155.0343,-299.0463 157.4815,-289.7729 159.761,-281.1347\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"163.1589,-281.9753 162.3264,-271.4133 156.3906,-280.1892 163.1589,-281.9753\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1661,-306.8314C237.5586,-297.0597 218.2058,-284.9432 201.815,-274.6811\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"203.3896,-271.5376 193.0564,-269.1975 199.6749,-277.4707 203.3896,-271.5376\"/>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"214.1855,-52 130.1855,-52 130.1855,-16 214.1855,-16 214.1855,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.1855\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">precision.txt</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;0 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M203.8034,-88.2022C198.9937,-79.9569 193.1754,-69.9828 187.848,-60.8501\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.8018,-58.9674 182.7398,-52.0931 184.7553,-62.4945 190.8018,-58.9674\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"297.6855,-52 232.6855,-52 232.6855,-16 297.6855,-16 297.6855,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.1855\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">recall.txt</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M226.5311,-88.5708C232.5349,-80.095 239.8782,-69.7279 246.5456,-60.3151\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"249.4967,-62.2039 252.4209,-52.0206 243.7846,-58.1578 249.4967,-62.2039\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f6d30ca53c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a graph representation of the precision artifact's lineage\n",
    "precision.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPROCESSING\n",
      "saving sparse matrices\n",
      "Loading Data\n",
      "Training Model\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'entropy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fc55b208404e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/flor/flor/object_model/artifact.py\u001b[0m in \u001b[0;36mpull\u001b[0;34m(self, manifest)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pull__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/flor/flor/shared_object_model/resource.py\u001b[0m in \u001b[0;36m__pull__\u001b[0;34m(self, pulled_object, manifest)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mconsolidated_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsolidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mVersioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpulled_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pull_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mPullTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/flor/flor/engine/executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(eg)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mrunning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/flor/flor/engine/executor.py\u001b[0m in \u001b[0;36m__run__\u001b[0;34m(eg, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkee\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d3eb2a46fd43>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(X_train, X_test, y_train, y_test, hyperparameters, precision, recall, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1, max_iter = 1000).fit(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#clf = MultinomialNB().fit(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#clf = KNeighborsClassifier().fit(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rise/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rise/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             splitter = SPLITTERS[self.splitter](criterion,\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                 \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entropy'"
     ]
    }
   ],
   "source": [
    "#Run the experiment\n",
    "precision.pull()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
