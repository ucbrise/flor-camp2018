{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp 2018: Introduction to Flor!\n",
    "\n",
    "\n",
    "Welcome to RISE Camp 2018! Flor is a system for managing workflow development within the machine learning lifecycle. This tool enables data scientists to describe ML workflows as directed acyclic graphs (DAGs) of Actions, Artifacts, or Literals and to experiment with different configurations quickly by running multi-trial experiments. \n",
    "\n",
    "The purpose of this notebook is to help you use Flor in order to naviagte through different parts of the data science lifecycle.\n",
    "\n",
    "As you work through this notebook, you will learn:\n",
    "\n",
    "* How to define/use experiments, literals, artifacts and actions.\n",
    "* How to run experiments with different congigurations.\n",
    "* Compare models with other past versions in order to select the best model.\n",
    "\n",
    "We will be working with a ratings dataset. \n",
    "\n",
    "**Data science is a collaborative activity - we encourage you to work with those around you and ask questions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scipy.sparse\n",
    "import flor\n",
    "\n",
    "#Pre-processing imports\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Model training and testing imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import average_precision_score, recall_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Before building our model, we will define a pipeline to pre-process our text data. We have used the following techniques to pre-process the text reviews:\n",
    "* Removal of Stop Words\n",
    "* Stemming (reducing inflected words to their stem)\n",
    "* Lemmatization (group together inflected forms of a words)\n",
    "\n",
    "We will wrap the following functions with the @flor.func decorator so they are able to be referenced by Flor actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def preprocessing(data_loc, intermediate_X, intermediate_y, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Data Preprocessing\n",
    "\n",
    "    '''\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "#     data = pd.read_json(data_loc)\n",
    "#     X = data['text']\n",
    "#     y = data['rating'].astype(np.float64)\n",
    "    \n",
    "#     en_stop = get_stop_words('en')\n",
    "\n",
    "#     def filter_sentence(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [word for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in en_stop]\n",
    "#         tokens = stem_words(tokens)\n",
    "#         tokens = lemma_words(tokens)\n",
    "\n",
    "#         ret_str = \" \".join(tokens) \n",
    "\n",
    "#         return ret_str \n",
    "\n",
    "\n",
    "#     #Credit to https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "#     #for stem_words and lemma_words\n",
    "#     def stem_words(words):\n",
    "#         stemmer = PorterStemmer()\n",
    "#         stems = []\n",
    "#         for word in words:\n",
    "#             stem = stemmer.stem(word)\n",
    "#             stems.append(stem)\n",
    "#         return stems\n",
    "\n",
    "#     def lemma_words(words):\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         lemmas = []\n",
    "#         for word in words:\n",
    "#             lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#             lemmas.append(lemma)\n",
    "#         return lemmas\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     X = [filter_sentence(el) for el in X]\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#     y_new = []\n",
    "#     for el in y:\n",
    "#         ret = 0\n",
    "#         if el <= 5:\n",
    "#             ret = 0\n",
    "#         else:\n",
    "#             ret = 1\n",
    "#         y_new.append(ret)\n",
    "#     y = y_new\n",
    "\n",
    "    # Load the cleaned data\n",
    "    with open('data_clean_X.json') as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open('data_clean_y.json') as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "\n",
    "    with open(intermediate_X, 'w') as outfile:\n",
    "       json.dump(X, outfile)\n",
    "    with open(intermediate_y, 'w') as outfile:\n",
    "       json.dump(y, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "We create a flor function to split our data into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def traintest_split(intermediate_X, intermediate_y, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Flor function to perform train/test split.\n",
    "\n",
    "    '''\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "#     def train_drop(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [el for el in tokens if random.random() >= 0.75]\n",
    "#         ret_str = \" \".join(tokens) \n",
    "#         return ret_str \n",
    "#     X_tr = [train_drop(el) for el in X_tr]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    start_time = time.time()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    with open(y_train, 'w') as outfile:\n",
    "        json.dump(y_tr, outfile)\n",
    "    with open(y_test, 'w') as outfile:\n",
    "        json.dump(y_te, outfile)\n",
    "\n",
    "    print(\"saving sparse matrices\")\n",
    "    scipy.sparse.save_npz(X_train, X_tr)\n",
    "    scipy.sparse.save_npz(X_test, X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Now, we can create a Flor function to train and evaluate a model to classify reviews into rating buckets. Notice that we pass in `hyperparameters` in addition to the train and test data. These hyperparameters will allow us to tune our model and track results with ease; we will define them later in our experiment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_test(X_train, X_test, y_train, y_test, hyperparameters, precision, recall, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Flor function to train and evaluate model.\n",
    "\n",
    "    '''\n",
    "    print(\"Loading Data\")\n",
    "    X_train = scipy.sparse.load_npz(X_train)\n",
    "    X_test = scipy.sparse.load_npz(X_test)\n",
    "    with open(y_train) as json_data:\n",
    "        y_train = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(y_test) as json_data:\n",
    "        y_test = json.load(json_data)\n",
    "        json_data.close()\n",
    "    print(\"Training Model\")\n",
    "    \n",
    "    #Either train Random Forest or Multi-layer Perception Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=hyperparameters).fit(X_train, y_train)\n",
    "    #clf.fit(X_train, y_train)\n",
    "    \n",
    "    #clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1).fit(X_train, y_train)\n",
    "    #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1).fit(X_train, y_train)    \n",
    "    #clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(2, ), random_state=1, max_iter = 1000).fit(X_train, y_train)\n",
    "    #clf = MultinomialNB().fit(X_train, y_train)\n",
    "    #clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    #clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "    print(\"Predicting Model\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Writing Results\") \n",
    "    \n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    #Write the precision to the output file\n",
    "    output = open(precision, 'w')\n",
    "    output.write(str(hyperparameters) + '\\n')\n",
    "    output.write(str(prec))\n",
    "    output.close()\n",
    "    \n",
    "    #Write the recall to the output file\n",
    "    output = open(recall, 'w')\n",
    "    output.write(str(hyperparameters) + '\\n')\n",
    "    output.write(str(rec))\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "Finally, we will now define our Flor experiment using the Flor functions we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment(\"risecamp_demo\") as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "    \n",
    "    # Defines preprocessing action and resulting intermediary artifacts\n",
    "    data = ex.artifact('data.json', 'data_loc')\n",
    "    preprocessing = ex.action(preprocessing, [data])\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', preprocessing)\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', preprocessing)\n",
    "    \n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "    \n",
    "    #Define the hyperparameters for the models\n",
    "    random_forest_Nestimators = ex.literalForEach(v=[25, 50, 75], name=\"hyperparameters\", default=50)\n",
    "    #MLP_hidden_layer_size = ex.literalForEach(v=[(1, ), (2, ), (3, )], name=\"hyperparameters\", default=(2, ))\n",
    "    \n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    #change to MLP_hidden_layer_size \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    #for naive bayes\n",
    "#     do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    precision = ex.artifact('precision.txt', 'precision', do_test)\n",
    "    recall = ex.artifact('recall.txt', 'recall', do_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a graph representation of the precision artifact's lineage\n",
    "precision.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPROCESSING\n",
      "saving sparse matrices\n",
      "Loading Data\n",
      "Training Model\n",
      "Predicting Model\n",
      "Writing Results\n",
      "Loading Data\n",
      "Training Model\n",
      "Predicting Model\n",
      "Writing Results\n",
      "Loading Data\n",
      "Training Model\n",
      "Predicting Model\n",
      "Writing Results\n"
     ]
    }
   ],
   "source": [
    "#Run the experiment\n",
    "precision.pull()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
