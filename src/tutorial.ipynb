{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/rise/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scipy.sparse\n",
    "import flor\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import average_precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.setNotebookName('tutorial.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def preprocessing(data_loc, intermediate_X, intermediate_y, **kwargs):\n",
    "    '''\n",
    "\n",
    "    Data Preprocessing\n",
    "\n",
    "    '''\n",
    "    data = pd.read_json(data_loc)\n",
    "    X = data['text']\n",
    "    y = data['rating'].astype(np.float64)\n",
    "    \n",
    "#     en_stop = get_stop_words('en')\n",
    "\n",
    "#     def filter_sentence(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [word for word in tokens if word.isalpha()]\n",
    "#         tokens = [word for word in tokens if word not in en_stop]\n",
    "#         tokens = stem_words(tokens)\n",
    "#         tokens = lemma_words(tokens)\n",
    "\n",
    "#         #tokens = [el for el in tokens if random.random() >= 0.75]\n",
    "#         ret_str = \" \".join(tokens) \n",
    "\n",
    "#         return ret_str \n",
    "\n",
    "\n",
    "#     #Credit to https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "#     #for stem_words and lemma_words\n",
    "#     def stem_words(words):\n",
    "#         stemmer = PorterStemmer()\n",
    "#         stems = []\n",
    "#         for word in words:\n",
    "#             stem = stemmer.stem(word)\n",
    "#             stems.append(stem)\n",
    "#         return stems\n",
    "\n",
    "#     def lemma_words(words):\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         lemmas = []\n",
    "#         for word in words:\n",
    "#             lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#             lemmas.append(lemma)\n",
    "#         return lemmas\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     X = [filter_sentence(el) for el in X]\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#     y_new = []\n",
    "#     for el in y:\n",
    "#         ret = 0\n",
    "#         if el <= 5:\n",
    "#             ret = 0\n",
    "#         else:\n",
    "#             ret = 1\n",
    "#         y_new.append(ret)\n",
    "#     y = y_new\n",
    "#     with open('data_clean_X.json', 'w') as outfile:\n",
    "#        json.dump(X, outfile)\n",
    "#     with open('data_clean_y.json', 'w') as outfile:\n",
    "#        json.dump(y, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# start_time = time.time()\n",
    "# X = vectorizer.fit_transform(X)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# print('Shape of Sparse Matrix: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def split(intermediate_X, intermediate_y, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    with open(intermediate_X) as json_data:\n",
    "        X = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(intermediate_y) as json_data:\n",
    "        y = json.load(json_data)\n",
    "        json_data.close()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "#     def train_drop(el):\n",
    "#         tokens = word_tokenize(el)\n",
    "#         tokens = [el for el in tokens if random.random() >= 0.75]\n",
    "#         ret_str = \" \".join(tokens) \n",
    "#         return ret_str \n",
    "#    X_train = [train_drop(el) for el in X_train]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    start_time = time.time()\n",
    "    vectorizer.fit(X_tr)\n",
    "    X_tr = vectorizer.transform(X_tr)\n",
    "    X_te = vectorizer.transform(X_te)\n",
    "    \n",
    "    scipy.sparse.save_npz(X_train, X_tr)\n",
    "    scipy.sparse.save_npz(X_test, X_te)\n",
    "    \n",
    "    with open(y_train, 'w') as outfile:\n",
    "        json.dump(y_tr, outfile)\n",
    "    with open(y_test, 'w') as outfile:\n",
    "        json.dump(y_te, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_test(X_train, X_test, y_train, y_test, **kwargs):\n",
    "    print(\"inside here!\")\n",
    "    X_train = scipy.sparse.load_npz(X_train)\n",
    "    X_test = scipy.sparse.load_npz(X_test)\n",
    "    with open(y_train) as json_data:\n",
    "        y_train = json.load(json_data)\n",
    "        json_data.close()\n",
    "    with open(y_test) as json_data:\n",
    "        y_test = json.load(json_data)\n",
    "        json_data.close()\n",
    "    print(\"received data\")\n",
    "    clf = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n",
    "    print(\"training\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"predicting\")\n",
    "    result = average_precision_score(y_pred, y_test)\n",
    "    \n",
    "    output = open(precision, 'w')\n",
    "    output.write(str(result))\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=92)\n",
    "\n",
    "# start_time = time.time()\n",
    "# clf = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print('\\n')\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with flor.Experiment(\"risecamp_demo\") as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "    \n",
    "    data = ex.artifact('data.json', 'data_loc')\n",
    "    preprocessing = ex.action(preprocessing, [data])\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', preprocessing)\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', preprocessing)\n",
    "    \n",
    "    do_split = ex.action(split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "    \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    precision = ex.artifact('precision.txt', 'precision', do_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-947eb3e59c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/flor/flor/object_model/artifact.py\u001b[0m in \u001b[0;36mpull\u001b[0;34m(self, manifest)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pull__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanifest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/flor/flor/shared_object_model/resource.py\u001b[0m in \u001b[0;36m__pull__\u001b[0;34m(self, pulled_object, manifest)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mconsolidated_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsolidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mVersioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpulled_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pull_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mPullTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsolidated_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/flor/flor/engine/executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(eg)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mrunning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;31m# TODO: Make for-loop concurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "precision.pull()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
