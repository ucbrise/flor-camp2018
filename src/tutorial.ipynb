{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp 2018: Introduction to Flor!\n",
    "\n",
    "\n",
    "Welcome to RISE Camp 2018! Flor is a system for managing workflow development within the machine learning lifecycle. This tool enables data scientists to describe ML workflows as directed acyclic graphs (DAGs) of Actions, Artifacts, or Literals and to experiment with different configurations quickly by running multi-trial experiments. \n",
    "\n",
    "The purpose of this notebook is to help you use Flor in order to naviagte through different parts of the data science lifecycle.\n",
    "\n",
    "As you work through this notebook, you will learn:\n",
    "\n",
    "* How to define/use experiments, literals, artifacts and actions.\n",
    "* How to run experiments with different congigurations.\n",
    "* Compare models with other past versions in order to select the best model.\n",
    "\n",
    "We will be working with a ratings dataset. Our goal is to predict whether a movie review is positive or negative based on its text.\n",
    "\n",
    "**Data science is a collaborative activity - we encourage you to work with those around you and ask questions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import scipy.sparse\n",
    "import flor\n",
    "\n",
    "#Pre-processing imports\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Model training and testing imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import average_precision_score, recall_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the notebook name has not already been set, you are able to set the name in code. \n",
    "flor.setNotebookName('tutorial.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Before building our model, we will define a pipeline to pre-process our text data. We have used the following techniques to pre-process the text reviews:\n",
    "* Removal of Stop Words\n",
    "* Stemming (reducing inflected words to their stem)\n",
    "* Lemmatization (group together inflected forms of a words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me\n",
    "from preprocess import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "We have created a function to split our data into train/test sets. Since we would like this to be a Flor Function, we must wrap it with the @flor.func decorator so it is able to be referenced by Flor actions. **Please navigate to the florfunctions.py file and wrap the `traintest_split` function with the Flor decorator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me\n",
    "from florfunctions import traintest_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Now, we can create a Flor function to train and evaluate a model to classify reviews into rating buckets. **Please navigate to the florfunctions.py file and complete the `train_test` function; fill in the Random Forest model with an n_estimators parameter of 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from florfunctions import train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "Finally, we will now define our Flor experiment using the Flor functions we created above. Proceed through the following cells and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = flor.Experiment('risecamp_demo').__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ex.artifact('data.json', 'data_loc')\n",
    "do_preproc = ex.action(preprocessing, [data,])\n",
    "data_x = ex.artifact('data_clean_X.json', 'intermediate_X', parent=do_preproc)\n",
    "data_y = ex.artifact('data_clean_y.json', 'intermediate_y', parent=do_preproc)\n",
    "ex.__exit__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing\n",
      "--- 197.00618505477905 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data_x.pull(utag='second_preproc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the data we want to analyze. We can load the data by creating **artifacts**, which are pointers to data we want. In this case, we have already generated cleaned data from a previous experiment run; we can retrieve the cleaned data by referencing the tag of the particular run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines preprocessing action and resulting intermediary artifacts\n",
    "#TODO: double check syntax\n",
    "data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag=\"first\")\n",
    "data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data points, we need to perform a train/test split. Using the `traintest_split` function we imported earlier, let's create a flor action as well as the intermediary artifacts generated by the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traintest_split is the function to run, data_x and data_y are arguments\n",
    "do_split = ex.action(traintest_split, [data_x, data_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artifacts have a pointer (filename), internal name, and (optional) parent\n",
    "X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "y_test = ex.artifact('y_test.json', 'y_test', do_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can specify the hyperparameter with a **literal**, an explicit value stored in Flor, and create an action for our `train_test` function and an artifact for our result. We can wrap up the experiment and close it with `__exit__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "#Define the model training and evaluation action and final artifacts\n",
    "do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, hyperparameter])\n",
    "report = ex.artifact('report.csv', 'report', do_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.Experiment(\"risecamp_demo\").__exit__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull\n",
    "\n",
    "Finally, we are ready to run the experiment! We can do so by running `pull()` on our output artifacts. Before doing this, however, it is helpful to use `plot()` to generate a florplan, a graph representation of the artifact's lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "report.pull(\"first_pull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Flor makes it convenient to run models using different hyperparameters and track the results. In the `train_test` we created function, notice that we pass in `hyperparameters` in addition to the train and test data. These hyperparameters will allow us to tune our model and track results with ease; let's define them in our experiment setup.\n",
    "\n",
    "Notice that the Random Forest Classifier contains `n_estimators` as a hyperparameter. We would like to tune this hyperparameter and track model performance. In order to specify the hyperparameters, we must make a `literalForEach` within our experiment. **Fill in the `literalForEach` with values 5, 50 and 75 within the experiment below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: We have copied the same experiment below for convenience.\n",
    "#We can also create flor experiments using a context manager.\n",
    "\n",
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "\n",
    "    # Defines artifacts\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='first')\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='first')\n",
    "\n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "\n",
    "    #Define the hyperparameters for the models\n",
    "    random_forest_Nestimators = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "#     random_forest_Nestimators = ex.literalForEach(v=[5, 50, 75], name=\"hyperparameters\", default=50) #SOLUTION\n",
    "\n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    report = ex.artifact('report.csv', 'report', do_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "report.pull(utag=\"hyperparameter_tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeking at Results\n",
    "\n",
    "After running the model with different hyperparameters above, we are able to peek at our output artifact, containing precision and recall metrics for the different models. Run the following cell - **which hyperparameter yields the best model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run me\n",
    "report.peek() #Depends on dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Better Model\n",
    "\n",
    "Now that you have some experience using flor, let's try using a different model to see if we can improve the results. Some of the classifiers we recommend trying are the Multilayer Perceptron Classifier, Naive Bayes Classifier, and K-neighbors Classifier.\n",
    "\n",
    "After implementing your model of choice in the `train_test` function in **florfunctions.py**, run the cells below to reimport the function and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from florfunctions import train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: We have copied the same experiment below for convenience.\n",
    "#We can also create flor experiments using a context manager.\n",
    "\n",
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "\n",
    "    # Defines artifacts\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='first')\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='first')\n",
    "\n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "\n",
    "    #Define the hyperparameters for the models\n",
    "    #hyperparameter = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "    random_forest_Nestimators = ex.literalForEach(v=[5, 50, 75], name=\"hyperparameters\", default=50) #SOLUTION\n",
    "    #MLP_hidden_layer_size = ex.literalForEach(v=[(1, ), (2, ), (3, )], name=\"hyperparameters\", default=(2, ))\n",
    "\n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    #change to MLP_hidden_layer_size \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    #for naive bayes\n",
    "    #do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    report = ex.artifact('report.csv', 'report', do_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report.pull(utag=\"improved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mishaps\n",
    "\n",
    "It turns out, the data that we have been working had not been cleaned optimally to begin with. In fact, we can observe the exact cleaning process in the `pre-processing` function within florfunctions.py. We can re-clean raw data by adding another flor action and intermediate artifacts. Fortunately, however, we already ran an experiment with more optimal preprocessing. We can checkout the artifacts by using `utag = 'second'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='first')\n",
    "data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the new data impacts our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: We have copied the same experiment below for convenience.\n",
    "\n",
    "# Create a context manager for the experiment and is named 'risecamp_demo'\n",
    "with flor.Experiment('risecamp_demo') as ex:\n",
    "    ex.groundClient('git') #use \"git\" from grit and \"ground\" for ground\n",
    "\n",
    "    # Defines artifacts\n",
    "    data_x = ex.artifact('data_clean_X.json', 'intermediate_X', utag='second')\n",
    "    data_y = ex.artifact('data_clean_y.json', 'intermediate_y', utag='second')\n",
    "\n",
    "    #Define split action and resulting output artifacts\n",
    "    do_split = ex.action(traintest_split, [data_x, data_y])\n",
    "    X_train = ex.artifact('x_train.npz', 'X_train', do_split)\n",
    "    X_test = ex.artifact('x_test.npz', 'X_test', do_split)\n",
    "    y_train = ex.artifact('y_train.json', 'y_train', do_split)\n",
    "    y_test = ex.artifact('y_test.json', 'y_test', do_split)\n",
    "\n",
    "    #Define the hyperparameters for the models\n",
    "    #hyperparameter = ex.literal(v = 5, name=\"hyperparameters\")\n",
    "    random_forest_Nestimators = ex.literalForEach(v=[5, 50, 75], name=\"hyperparameters\", default=50) #SOLUTION\n",
    "    #MLP_hidden_layer_size = ex.literalForEach(v=[(1, ), (2, ), (3, )], name=\"hyperparameters\", default=(2, ))\n",
    "\n",
    "    #Define the model training and evaluation action and final artifacts\n",
    "    #change to MLP_hidden_layer_size \n",
    "    do_test = ex.action(train_test, [X_train, X_test, y_train, y_test, random_forest_Nestimators])\n",
    "    #for naive bayes\n",
    "    #do_test = ex.action(train_test, [X_train, X_test, y_train, y_test])\n",
    "    report = ex.artifact('report.csv', 'report', do_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.pull(utag=\"better_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
